---
title: "MCO: primeros pasos"
subtitle: "Econometría I"
author: "Paula Pereda (ppereda@correo.um.edu.uy)"
date: "27 de agosto de 2021"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}
options(htmltools.dir.version = FALSE)

knitr::knit_engines$set(upper = function(options) {
  code <- paste(options$code, collapse = "\n")
  if (options$eval) 
    toupper(code) else code
})

library(pacman)
p_load(broom, latex2exp, ggplot2, ggthemes, viridis, dplyr, magrittr, knitr, parallel)
# Define pink color
red_pink <- "#e64173"
# Notes directory
# Knitr options
opts_chunk$set(
  comment = ">",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  # dpi = 300,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 14),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_empty + theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
)
```


```{R, colors, include = F}
# Define pink color
red_pink <- "#e64173"
turquoise <- "#20B2AA"
orange <- "#FFA500"
red <- "#fb6107"
blue <- "#2b59c3"
green <- "#8bb174"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
purple <- "#6A5ACD"
```

layout: false
class: inverse, middle
# Repaso

---
layout: true
# Población *vs.* muestra

---

## Modelos y notación

Escribimos nuestro modelo poblacional (simple) así:

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$

y nuestro modelo de regresión estimado basado en la muestra así:

$$ y_i = \hat{\beta}_0 + \hat{\beta}_1 x_i + e_i $$

Un modelo estimado de regresión produce estimaciones para cada observación:

$$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$

lo que nos da la línea que _mejor se ajusta_ a nuestra muestra.

---
layout: true

# Población *vs.* muestra

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?

---

--

```{R, gen dataset, include = F, cache = T}
# Set population y sample sizes
n_p <- 100
n_s <- 30
# Set the seed
set.seed(1989)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(1989)
sim_df <- mclapply(mc.cores = 10, X = 1:1e4, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

.pull-left[

```{R, pop1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_empty
```

.center[**Población**]

]

--

.pull-right[

```{R, scatter1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_empty
```

.center[**Relación poblacional**]

$$ y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[

```{R, sample1, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 1:** 30 individuos aleatorios]

]

--

.pull-right[

```{R, sample1 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relación poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relación muestral**
<br>
$\hat{y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` x_i$

]

]

---
count: false

.pull-left[

```{R, sample2, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 2:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample2 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relación poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relación muestral**
<br>
$\hat{y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` x_i$

]

]
---
count: false

.pull-left[

```{R, sample3, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[**Muestra 3:** 30 individuos aleatorios]

]

.pull-right[

```{R, sample3 scatter, echo = F, fig.fullwidth = T, dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_empty
```

.center[

**Relación poblacional**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Relación muestral**
<br>
$\hat{y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` x_i$

]

]

---
layout: false
class: clear, middle

Vamos a repetir esto **10.000 veces**.

(Este ejercicio se llama simulación Monte Carlo.)

---
layout: true
# Población *vs.* muestra

---

```{R, simulation scatter, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01) +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 1.5
) +
theme_empty
```

---
layout: true
# Población *vs.* muestra

---

.pull-left[
```{R, simulation scatter2, echo = F, dev = "png", dpi = 300, cache = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01, size = 1) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3
) +
theme_empty
```
]

.pull-right[

- En **promedio**, nuestras líneas de regresión coinciden muy bien con la línea poblacional.

- Sin embargo, **líneas individuales** (muestras) realmente pueden fallar.

- Las diferencias entre las muestras individuales y la población generan **incertidumbre** para el o la econometrista.
]

---

**Pregunta:** ¿Por qué nos importa la *población vs. muestra*?

--

**Respuesta:** La incertidumbre importa.

$\hat{\beta}$ en sí mismo es una variable aleatoria, que depende de la muestra aleatoria. Cuando tomamos una muestra y corremos una regresión, no sabemos si es una 'buena' muestra, $\hat{\beta}$ está cerca de $\beta$, o una 'mala muestra', nuestra muestra difiere mucho de la población.

---
layout: false
# Población *vs.* muestra

## Incertidumbre

Hacer un seguimiento de esta incertidumbre será clave en el curso.

- Estimación de errores estándar para nuestras estimaciones.

- Prueba de hipótesis.

- Corrección de heterocedasticidad y autocorrelación.

--

Primero, repasemos cómo obtenemos estas estimaciones de regresión (inciertas).

---
# Regresión lineal

## El estimador

Podemos estimar la línea de regresión en .mono[R] (`lm(y ~ x, my_data)`) pero... ¿de dónde vienen estas estimaciones? 


Unas diapositivas atrás:

> $$ \hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i $$
> lo que nos da la línea que *mejor se ajusta* a nuestra muestra (set de datos).

¿Pero a qué nos referimos con la "línea que mejor se ajusta"?

---
layout: false

# Siendo el "mejor"

**Pregunta:** ¿A qué nos referimos con la *línea que mejor se ajusta*?

**Respuestas:**

- En general (en econometría), la *línea que mejor se ajusta* significa la línea que minimiza la suma de minimizando la suma de errores o residuos al cuadrado (SCR):

.center[

$\text{SCR} = \sum_{i = 1}^{n} e_i^2\quad$ donde $\quad e_i = y_i - \hat{y}_i$

]

- **Mínimos Cuadrados** Ordinarios (**MCO**) minimiza la suma de errores o residuos al cuadrado.
- Basado en un conjunto de supuestos (en su mayoría aceptables), MCO
  - Es insesgado (y consistente)
  - Es el *mejor* (mínima varianza) estimador lineal insesgado (MELI)

---
layout: true
# MCO *vs.* otras líneas/estimadores

---

Consideramos el set de datos generado anteriormente.

```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 6}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier línea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$

```{R, vs lines 2, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier línea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier línea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

Para cualquier línea $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, podemos calcular los errores: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
theme_empty
```

---
count: false

La SCR expresa los errores al cuadrado $\left(\sum e_i^2\right)$: mayores errores, reciben mayores penalizaciones.

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
count: false

El estimador de MCO es una combinación de $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimizan la SCR.

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 6}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = red_pink, size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty
```

---
layout: true
# MCO

## Formalmente

---

En una regresión lineal simple, el estimador de MCO proviene de escoger los $\hat{\beta}_0$ y $\hat{\beta}_1$ que minimizan la suma de errores o residuos al cuadrado (SCR), _es decir_,

$$\min_{\hat{\beta}_0,\, \hat{\beta}_1} \text{SCR}$$

--

pero ya sabemos que $\text{SCR} = \sum_i e_i^2$, ahora usamos las definiciones de $e_i$ y $\hat{y}$.

$$
\begin{aligned}
  e_i^2 &= \left( y_i - \hat{y}_i \right)^2 = \left( y_i - \hat{\beta}_0 - \hat{\beta}_1 x_i \right)^2 \\
  &= y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2
\end{aligned}
$$

--

**Recuerden:** Minimizar una función multivariante requiere (**1**) primeras derivadas iguales a cero (las *condiciones de primer orden*) y (**2**) condiciones de segundo orden (concavidad).

---

Nos estamos acercando. Necesitamos **minimizar las SCR**. Hemos mostrado cómo SCR se relaciona con nuestra muestra (nuestros datos: $x$ y $y$) y nuestras estimaciones, _es decir_, $\hat{\beta}_0$ y $\hat{\beta}_1$.

$$ \text{SCR} = \sum_i e_i^2 = \sum_i \left( y_i^2 - 2 y_i \hat{\beta}_0 - 2 y_i \hat{\beta}_1 x_i + \hat{\beta}_0^2 + 2 \hat{\beta}_0 \hat{\beta}_1 x_i + \hat{\beta}_1^2 x_i^2 \right) $$

Para las condiciones de primer orden de minimización, tomamos la primera derivada de la SCR con respecto $\hat{\beta}_0$ y $\hat{\beta}_1$.

$$
\begin{aligned}
  \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_0} &= \sum_i \left( 2 \hat{\beta}_0 + 2 \hat{\beta}_1 x_i - 2 y_i \right) = 2n \hat{\beta}_0 + 2 \hat{\beta}_1 \sum_i x_i - 2 \sum_i y_i \\
  &= 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y}
\end{aligned}
$$

donde $\overline{x} = \frac{\sum x_i}{n}$ y $\overline{y} = \frac{\sum y_i}{n}$ son medias muestrales de $x$ y $y$ (tamaño $n$).

---

Las condiciones de primer orden establecen que las derivadas son iguales a cero, entonces:

$$ \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_0} = 2n \hat{\beta}_0 + 2n \hat{\beta}_1 \overline{x} - 2n \overline{y} = 0 $$

lo que implica que

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ahora para $\hat{\beta}_1$.

---

Tomamos la derivada de SCR con respecto a $\hat{\beta}_1$

$$
\begin{aligned}
  \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_1} &= \sum_i \left( 2 \hat{\beta}_0 x_i + 2 \hat{\beta}_1 x_i^2 - 2 y_i x_i \right) = 2 \hat{\beta}_0 \sum_i x_i + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i \\
  &= 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i
\end{aligned}
$$

lo igualamos a cero (condiciones de primer orden, de nuevo 😅)

$$ \dfrac{\partial \text{SCR}}{\partial \hat{\beta}_1} = 2n \hat{\beta}_0 \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

y sustituimos en nuestra relación por $\hat{\beta}_0$, _es decir_, $\hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x}$. Entonces,

$$
 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0
$$

---

Continuando de la diapositiva anterior

$$ 2n \left(\overline{y} - \hat{\beta}_1 \overline{x}\right) \overline{x} + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

multiplicamos

$$ 2n \overline{y}\,\overline{x} - 2n \hat{\beta}_1 \overline{x}^2 + 2 \hat{\beta}_1 \sum_i x_i^2 - 2 \sum_i y_i x_i = 0 $$

$$ \implies 2 \hat{\beta}_1 \left( \sum_i x_i^2 - n \overline{x}^2 \right) = 2 \sum_i y_i x_i - 2n \overline{y}\,\overline{x} $$

$$ \implies \hat{\beta}_1 = \dfrac{\sum_i y_i x_i - 2n \overline{y}\,\overline{x}}{\sum_i x_i^2 - n \overline{x}^2} = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

---

¡Listo! 

Ahora tenemos adorables estimadores de MCO para la pendiente

$$ \hat{\beta}_1 = \dfrac{\sum_i (x_i - \overline{x})(y_i - \overline{y})}{\sum_i (x_i - \overline{x})^2} $$

y el intercepto

$$ \hat{\beta}_0 = \overline{y} - \hat{\beta}_1 \overline{x} $$

Ahora sabemos de donde viene la parte de *mínimos cuadrados* de Mínimos Cuadrados Ordinarios 🤓🎊

--

Pasamos ahora a los supuestos y propiedades (implícitas) de MCO.

---
layout: false
class: inverse, middle

# MCO: Supuestos y propiedades

---
layout: true
# MCO: Supuestos y propiedades

## Propiedades
---

**Pregunta:** ¿Qué propiedades nos podrían interesar para un estimador?

--

**Tangente:** Revisemos primero las propiedades estadísticas.

---

**Refrescando:** Funciones de densidad

Recuerden que usamos **funciones de densidad de probabilidad** para describir la probabilidad que una **variable aleatoria continua** adopte un rango de valores. (El área total = 1)

Estas caracterizan distribuciones de probabilidad, y las distribuciones más comunes/famosas/populares obtienen nombres (_por ejemplo_, normal, *t*, Gamma).

---

**Refrescando:** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor entre -2 y 0: $\mathop{\text{P}}\left(-2 \leq X \leq 0\right) = 0.48$

```{R, example: pdf, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -2, 0)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Refrescando:** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar tome un valor entre -1.96 y 1.96: $\mathop{\text{P}}\left(-1.96 \leq X \leq 1.96\right) = 0.95$

```{R, example: pdf 2, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, -1.96, 1.96)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

**Refrescando:** Funciones de densidad

La probabilidad de que una variable aleatoria normal estándar adquiera un valor superior a 2: $\mathop{\text{P}}\left(X > 2\right) = 0.023$

```{R, example: pdf 3, echo = F, dev = "svg", fig.height = 3.5}
# Generate data for density's polygon
tmp <- tibble(x = seq(-4, 4, 0.01), y = dnorm(x))
tmp <- rbind(tmp, tibble(x = seq(4, -4, -0.01), y = 0))
# Plot it
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = "grey85") +
geom_polygon(data = tmp %>% filter(between(x, 2, Inf)), fill = red_pink) +
geom_hline(yintercept = 0, color = "black") +
theme_simple
```

---

Imagine que estamos tratando de estimar un parámetro desconocido $\beta$, y conocemos las distribuciones de tres estimadores en competencia. ¿Cuál querríamos? ¿Cómo decidiríamos?

```{R, competing pdfs, echo = F, dev = "svg", fig.height = 4.5}
# Generate data for densities' polygons
d1 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 1, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d2 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dunif(x, min = -2.5, max = 1.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d3 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2.5)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
# Plot them
ggplot() +
geom_polygon(data = d1, aes(x, y), alpha = 0.8, fill = "orange") +
geom_polygon(data = d2, aes(x, y), alpha = 0.65, fill = red_pink) +
geom_polygon(data = d3, aes(x, y), alpha = 0.6, fill = "darkslategray") +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Pregunta:** ¿Qué propiedades nos podrían interesar para un estimador?

--

**Respuesta uno: Sesgo.**

En promedio (después de *muchas* muestras), ¿el estimador tiende hacia el valor correcto?

**Más formalmente:** ¿La media de la distribución del estimador es igual al parámetro que se estima?

$$ \mathop{\text{Sesgo}}_\beta \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] - \beta $$

---

**Respuesta uno: Sesgo.**

.pull-left[

**Estimador insesgado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] = \beta$

```{R, unbiased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(fill = red_pink, alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]

--

.pull-right[

**Estimador sesgado:** $\mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \neq \beta$

```{R, biased pdf, echo = F, dev = "svg"}
ggplot(data = tmp, aes(x, y)) +
geom_polygon(aes(x = x + 2), fill = "darkslategray", alpha = 0.9) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 40))
```

]

---

**Respuesta dos: Varianza.**

Las tendencias centrales (medias) de distribuciones en competencia no son las únicas cosas que importan. También nos preocupamos por la **varianza** de un estimador.

$$ \mathop{\text{Var}} \left( \hat{\beta} \right) = \mathop{\boldsymbol{E}}\left[ \left( \hat{\beta} - \mathop{\boldsymbol{E}}\left[ \hat{\beta} \right] \right)^2 \right] $$

Los estimadores de varianza más baja significan que obtenemos estimaciones más cercanas a la media en cada muestra.

---
count: false

**Respuesta dos: Varianza.**

```{R, variance pdf, echo = F, dev = "svg", fig.height = 5}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---

**Respuesta uno: Sesgo.**

**Respuesta dos: Varianza.**

**Sútilmente:** El trade-off sesgo-varianza.

¿Deberíamos estar dispuestos a tomar un poco de sesgo para reducir la varianza?

En econometría, generalmente nos apegamos a estimadores insesgados (o consistentes). Pero otras disciplinas (especialmente las ciencias de la computación) piensan un poco más en esta compensación.

---
layout: false

# El trade-off sesgo-varianza.

```{R, variance bias, echo = F, dev = "svg"}
d4 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0.3, sd = 1)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
d5 <- tibble(x = seq(-7.5, 7.5, 0.01), y = dnorm(x, mean = 0, sd = 2)) %>%
  rbind(., tibble(x = seq(7.5, -7.5, -0.01), y = 0))
ggplot() +
geom_polygon(data = d4, aes(x, y), fill = red_pink, alpha = 0.9) +
geom_polygon(data = d5, aes(x, y), fill = "darkslategray", alpha = 0.8) +
geom_hline(yintercept = 0, color = "black") +
geom_vline(xintercept = 0, size = 1, linetype = "dashed") +
scale_x_continuous(breaks = 0, labels = TeX("$\\beta$")) +
theme_simple +
theme(axis.text.x = element_text(size = 20))
```

---
# MCO: Supuestos y propiedades

## Propiedades

Como ya habrán adivinado,

- MCO es **insesgado**.
- MCO tiene la **varianza mínima** de todos los estimadores lineales insesgados.

---
# MCO: Supuestos y propiedades

## Propiedades

Pero... estas (muy buenas) propiedades dependen de un conjunto de supuestos:

1. La relación de población es lineal en parámetros con una perturbación aditiva.

2. Nuestra variable $X$ es **exógena**, _es decir_, $\mathop{\boldsymbol{E}}\left[u \mid X \right] = 0$.

3. La variable $X$ tiene variación. Y si hay múltiples variables explicativas, no son perfectamente colineales.

4. Las perturbaciones de la población $u_i$ se distribuyen de forma independiente e idéntica como variables aleatorias normales con una media de cero $\left(\mathop{\boldsymbol{E}} \left[u \right] = 0 \right) $ y varianza $\sigma^2$ (_es decir_, $\mathop{\boldsymbol{E}} \left[u^2\right] = \sigma^2$). Independientemente distribuidos y significan cero conjuntamente implican $\mathop{\boldsymbol{E}} \left[u_i u_j\right] = 0$ para cualquier $i \neq j$.

---
# MCO: Supuestos y propiedades

## Supuestos

Diferentes supuestos garantizan diferentes propiedades:

- Supuestos (1), (2), y (3) hace al insesgado MCO.
- Supuesto (4) nos da un estimador insesgado para la varianza de nuestro estimador MCO.

Durante nuestro curso, discutiremos las muchas formas en que la vida real puede **violar estas suposiciones**. Por ejemplo:

- Relaciones no lineales en nuestros parámetros/perturbaciones (o errores de especificación).
- Perturbaciones que no se distribuyen de forma idéntica y/o no son independientes.
- Violaciones de exogeneidad (especialmente sesgo de variable omitida).

---
# MCO: Supuestos y propiedades

## Expectativa condicional

Para muchas aplicaciones, nuestro supuesto más importante es **exogeneidad**, _es decir_,
$$
\begin{align}
  \mathop{E}\left[ u \mid X \right] = 0
\end{align}
$$

¿pero qué significa esto?

--

Una forma de pensar en esta definición:

> Para *cualquier* valor de $X$, la media de los residuos debe ser cero.

- _Por ejemplo_, $\mathop{E}\left[ u \mid X=1 \right]=0$ *y* $\mathop{E}\left[ u \mid X=100 \right]=0$

- _Por ejemplo_, $\mathop{E}\left[ u \mid X_2=\text{Mujer} \right]=0$ *y* $\mathop{E}\left[ u \mid X_2=\text{Varón} \right]=0$

- Aviso: $\mathop{E}\left[ u \mid X \right]=0$ es más restrictiva que $\mathop{E}\left[ u \right]=0$
---
layout: false
class: clear, middle

Gráficamente...
---
exclude: true

```{R, conditional_expectation_setup, include = F, cache = T}

# Setup ----------------------------------------------------------------------------------
  # Options
  options(stringsAsFactors = F)
  # Packages
  library(pacman)
  p_load(ggridges)

# Data work ------------------------------------------------------------------------------
  # Set seed
  set.seed(1989)
  # Sample size
  n <- 1e5
  # Exogenous
  e_good <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n)
  ) %>% mutate(x = round(x))
  # Endogenous
  e_bad <- tibble(
    x = runif(n = n, min = -4, max = 4),
    e = rnorm(n) + 0.5 * x
  ) %>% mutate(x = round(x))

# Figures: Joint densities ---------------------------------------------------------------
  # The joint plot: good
  joint_good <- ggplot(data = e_good, aes(x = e)) +
    geom_density() +
    theme_pander()
  # The joint plot: bad
  joint_bad <- ggplot(data = e_bad, aes(x = e)) +
    geom_density() +
    theme_pander()

# Figures: Conditional densities ---------------------------------------------------------
  cond_good <- ggplot(data = e_good, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("u") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
  cond_bad <- ggplot(data = e_bad, aes(x = e, y = as.factor(x))) +
    geom_density_ridges_gradient(
      aes(fill = ..x..),
      color = "white",
      scale = 2.5,
      size = 0.2
    ) +
    # geom_vline(xintercept = 0, alpha = 0.3) +
    scale_fill_viridis(option = "magma") +
    xlab("u") +
    ylab("X") +
    theme_pander(base_family = "Fira Sans Book", base_size = 18) +
    theme(
      legend.position = "none",
      axis.title.y = element_text(angle = 0, vjust = 0.5, family = "MathJax_Math", size = 22),
      axis.title.x = element_text(family = "MathJax_Math", size = 22)
    )
```
---
class: clear

La exogeneidad válida, _es decir_, $\mathop{E}\left[ u \mid X \right] = 0$

```{R, ex_good_exog, echo = F, dev = "svg"}
cond_good
```
---
class: clear

La exogeneidad inválida, _es decir_, $\mathop{E}\left[ u \mid X \right] \neq 0$

```{R, ex_bad_exog, echo = F, dev = "svg"}
cond_bad
```


---
layout: false
class: inverse, middle
# Incertidumbre e inferencia

---
layout: true
# Incertidumbre e inferencia

---

## ¿Hay algo más?

Hasta este punto, sabemos que MCO tiene algunas propiedades interesantes, y sabemos cómo estimar un coeficiente de pendiente de intersección y a través de MCO.

Nuestro flujo de trabajo actual:
- Obtener datos (puntos con valores $x$ y $y$)
- Regresar $y$ en $x$
- Trace la línea MCO (_es decir_, $\hat {y} = \hat {\beta}_0 + \hat{\beta}_1 $)
- ¿Hecho?

Pero, ¿cómo podemos **aprender** algo de este ejercicio?
---

## Hay más

Pero, ¿cómo podemos **aprender** algo de este ejercicio?

- Con base en nuestro valor de $\hat{\beta}_1$, ¿podemos descartar valores hipotetizados previamente?
- ¿Qué confianza debemos tener en la precisión de nuestras estimaciones?
- ¿Qué tan bien explica nuestro modelo la variación que observamos en $y$?

Necesitamos poder lidiar con la incertidumbre. Ingresa en escena: **La inferencia.**

---
layout: true
# Incertidumbre e inferencia
## Aprendiendo de nuestros errores

---

Como señaló nuestra simulación anterior, nuestro problema con la **incertidumbre** es que no sabemos si nuestra estimación muestral está *cerca* o *lejos* del parámetro de población desconocido. <sup> .pink[†] </sup>

Sin embargo, no todo está perdido. Podemos usar los errores $\left (e_i = y_i - \hat{y}_i \right)$ para tener una idea de qué tan bien nuestro modelo explica la variación observada en $y$.

Cuando nuestro modelo parece estar haciendo un trabajo "agradable", es posible que tengamos un poco más de confianza al usarlo para conocer la relación entre $y$ y $x$.

Ahora solo tenemos que formalizar lo que realmente significa un "buen trabajo".

.note[.pink [†]: Excepto cuando corremos la simulación nosotros mismos, por eso nos gustan las simulaciones 😉]

---

En primer lugar, estimaremos la varianza de $u_i$ (recuerde: $\mathop{\text{Var}} \left (u_i \right) = \sigma^ 2$) usando nuestros errores al cuadrado, _es decir_,

$$s^2 = \dfrac{\sum_i e_i^2} {n - k}$$

donde $k$ da el número de términos de pendiente y intersecciones que estimamos (_por ejemplo_, $\beta_0$ y $\beta_1$ daría $k = 2$).

$s^2$ es un estimador insesgado de $\ sigma^2$.

---

Luego demostramos que la varianza de $\hat {\beta}_1$ (para regresión lineal simple) es

$$\mathop {\text {Var}} \left (\hat {\beta} _1 \right) = \dfrac {s^2} {\sum_i \left (x_i - \overline {x} \right)^ 2}$$

lo que muestra que la varianza de nuestro estimador de pendiente:

1. aumenta a medida que nuestras perturbaciones se vuelven más ruidosas
2. disminuye a medida que aumenta la varianza de $x$

---

*Más comúnmente:* El **error estándar** de $\hat{\beta}_1$

$$ \mathop{\hat{\text{EE}}} \left( \hat{\beta}_1 \right) = \sqrt{\dfrac{s^2}{\sum_i \left( x_i - \overline{x} \right)^2}} $$

*Recuerden:* El error estándar de un estimador es la desviación estándar de la distribución del estimador.

---

El error estándar en .mono[R]'s `lm`, se ve así:

```{R, se}
tidy(lm(y ~ x, pop_df))
```


---
layout: false
class: inverse, middle
# Interpretando coeficientes

---
layout: true
# Interpretando coeficientes

---
## Variables continuas

Consideramos la siguiente relación:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educación}_i + u_i $$

donde

- $\text{salario}_i$ es una variable continua que mide el salario de cada individuo
- $\text{educación}_i$ es una variable continua que mide los años de educación de cada persona

--

**Interpretaciones**

- $\beta_0$: es el intercepto de $y$, _es decir_, el $\text{salario}$ cuando $\text{educación} = 0$
- $\beta_1$: el aumento esperado en el $\text{salario}$ para un aumento unitario de $\text{educación}$

---
## Variables continuas

Considerando el siguiente modelo:

$$ y = \beta_0 + \beta_1 \, x + u $$

Diferencio el modelo:

$$ \dfrac{dy}{dx} = \beta_1 $$

_Es decir_, la pendiente nos dice el aumento esperado en la variable dependiente para un aumento en una unidad de la variable explicativa, **manteniendo todas las demás variables constantes** (*ceteris paribus*).
---
## Variables binarias

Considero la relación:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{mujer}_i + u_i $$

donde

- $\text{salario}_i$ es una variable continua que mide el salario de cada individuo
- $\text{mujer}_i$ es un variable binaria que toma el valor $1$ cuando $i$ es mujer

--

**Interpretaciones**

- $\beta_0$: el $\text{salario}$ esperado para los hombres (cuando $\text{mujer} = 0$)
- $\beta_1$: la diferencia esperada en el $\text{salario}$ entre hombres y mujeres
- $\beta_0 + \beta_1$: el $\text{salario}$ esperado para mujeres

---
## Variables binarias

Derivación:

$$
\begin{aligned}
 \mathop{\boldsymbol{E}}\left[ \text{salario} | \text{hombre} \right] &=
 \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1\times 0 + u_i \right] \\
 &= \mathop{\boldsymbol{E}}\left[ \beta_0 + 0 + u_i \right] \\
 &= \beta_0
\end{aligned}
$$

--

$$
\begin{aligned}
 \mathop{\boldsymbol{E}}\left[ \text{salario} | \text{mujer} \right] &=
 \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1\times 1 + u_i \right] \\
 &= \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 + u_i \right] \\
 &= \beta_0 + \beta_1
\end{aligned}
$$

--

**Nota:** Si no hay variables adicionales de control, entonces $\hat{\beta}_1$ equivale a la diferencia entre las medias de los grupos, _en este caso_, $\overline{x}_\text{mujer} - \overline{x}_\text{hombre}$.

--

**Nota<sub>2</sub>:** El *manteniendo todo lo demás constante* también aplica en regresiones que tienen variables binarias.

---
## Variables binarias

$y_i = \beta_0 + \beta_1 x_i + u_i$ para la variable binaria $x_i = \{\color{#314f4f}{0}, \, \color{#e64173}{1}\}$

```{R, cat data, include = F}
# Set seed
set.seed(1235)
# Sample size
n <- 5e3
# Generate data
cat_df <- tibble(
  x = sample(x = c(0, 1), size = n, replace = T),
  y = 3 + 7 * x + rnorm(n, sd = 2)
)
# Regression
cat_reg <- lm(y ~ x, data = cat_df)
```

```{R, dat plot 1, echo = F, dev = "svg", fig.height = 5.5}
set.seed(12345)
ggplot(data = cat_df, aes(x = x, y = y, color = as.factor(x))) +
geom_jitter(width = 0.3, size = 1.5, alpha = 0.5) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
theme_empty
```

---
## Variables binarias

$y_i = \beta_0 + \beta_1 x_i + u_i$ para la variable binarias $x_i = \{\color{#314f4f}{0}, \, \color{#e64173}{1}\}$

```{R, dat plot 2, echo = F, dev = "svg", fig.height = 5.5}
set.seed(12345)
ggplot(data = cat_df, aes(x = x, y = y, color = as.factor(x))) +
geom_jitter(width = 0.3, size = 1.5, alpha = 0.5) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
geom_hline(yintercept = cat_reg$coefficients[1], size = 1, color = "darkslategrey") +
geom_hline(yintercept = cat_reg$coefficients[1] + cat_reg$coefficients[2], size = 1, color = red_pink) +
annotate(
  geom = "text",
  x = 0.5,
  y = -1 + cat_reg$coefficients[1],
  label = TeX("$\\hat{\\beta}_0 = \\bar{\\mathrm{Grupo}_0}$"),
  size = 7
) +
annotate(
  geom = "text",
  x = 0.5,
  y = 1 + cat_reg$coefficients[1] + cat_reg$coefficients[2],
  label = TeX("$\\hat{\\beta}_0 + \\hat{\\beta}_1 = \\bar{\\mathrm{Grupo}_1}$"),
  size = 7,
  color = red_pink
) +
theme_empty
```
---
## Interacciones

Las interacciones permiten que el efecto de una variable cambie según el nivel de otra variable.

** Ejemplos **

1. ¿Cambia el efecto de la escolarización en el salario por sexo?

2. ¿El efecto del género en el salario cambia según la etnia?

3. ¿Cambia el efecto de la escolarización en el salario según la experiencia?
---

## Interacciones

Anteriormente, considerábamos un modelo que permitía a mujeres y hombres tener salarios diferentes, pero el modelo asumía que el efecto de la escuela sobre el salario era el mismo para todos:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educación}_i + \beta_2 \, \text{mujer}_i + u_i $$

pero también podemos permitir que el efecto de la escuela varíe según el sexo:

$$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educación}_i + \beta_2 \, \text{mujer}_i + \beta_3 \, \text{educación}_i\times\text{mujer}_i + u_i $$

---

## Interacciones

El modelo donde la escolarización tiene el mismo efecto para todos (**<font color="#e64173">M</font>** y **<font color="#314f4f">H</font>**):

```{R, int data, include = F, cache = T}
# Set seed
set.seed(1989)
# Sample size
n <- 1e3
# Parameters
beta0 <- 20; beta1 <- 0.5; beta2 <- 10; beta3 <- 3
# Dataset
int_df <- tibble(
  hombre = sample(x = c(F, T), size = n, replace = T),
  educación = runif(n, 3, 9) - 3 * hombre,
  salario = beta0 + beta1 * educación + beta2 * hombre + rnorm(n, sd = 7) + beta3 * hombre * educación
)
reg_noint <- lm(salario ~ educación + hombre, int_df)
reg_int <- lm(salario ~ educación + hombre + educación:hombre, int_df)
```

```{R, int plot 1, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = int_df, aes(x = educación, y = salario)) +
geom_point(aes(color = hombre, shape = hombre), size = 2.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_abline(
  intercept = reg_noint$coefficients[1] + reg_noint$coefficients[3],
  slope = reg_noint$coefficients[2],
  color = "darkslategrey", size = 1, alpha = 0.8
) +
geom_abline(
  intercept = reg_noint$coefficients[1],
  slope = reg_noint$coefficients[2],
  color = red_pink, size = 1, alpha = 0.8
) +
xlab("educación") +
ylab("salario") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Mujer", "Hombre")) +
scale_shape_manual("", values = c(16, 1), labels = c("Mujer", "Hombre"))
```

---

## Interacciones

El modelo donde el efecto de la educación puede variar por sexo (**<font color="#e64173">M</font>** y **<font color="#314f4f">H</font>**):

```{R, int plot 2, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = int_df, aes(x = educación, y = salario)) +
geom_point(aes(color = hombre, shape = hombre), size = 2.5) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_abline(
  intercept = reg_noint$coefficients[1] + reg_noint$coefficients[3],
  slope = reg_noint$coefficients[2],
  color = "darkslategrey", size = 0.75, alpha = 0.2
) +
geom_abline(
  intercept = reg_noint$coefficients[1],
  slope = reg_noint$coefficients[2],
  color = red_pink, size = 0.75, alpha = 0.2
) +
geom_abline(
  intercept = reg_int$coefficients[1] + reg_int$coefficients[3],
  slope = reg_int$coefficients[2] + reg_int$coefficients[4],
  color = "darkslategrey", size = 1, alpha = 0.8
) +
geom_abline(
  intercept = reg_int$coefficients[1],
  slope = reg_int$coefficients[2],
  color = red_pink, size = 1, alpha = 0.8
) +
xlab("educación") +
ylab("salario") +
theme_empty +
theme(
  axis.title = element_text(size = 18),
  plot.margin = structure(c(0, 0, 0.1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
) +
scale_color_manual("", values = c(red_pink, "darkslategrey"), labels = c("Mujer", "Hombre")) +
scale_shape_manual("", values = c(16, 1), labels = c("Mujer", "Hombre"))
```

<!-- --- -->
<!-- ## Interactions -->

<!-- Interpreting coefficients can be a little tricky with interactions, but the key<sup>.pink[†]</sup> is to carefully work through the math. -->

<!-- .footnote[.pink[†] As is often the case with econometrics.] -->

<!-- $$ \text{salario}_i = \beta_0 + \beta_1 \, \text{educación}_i + \beta_2 \, \text{Female}_i + \beta_3 \, \text{educación}_i\times\text{Female}_i + u_i $$ -->

<!-- Expected returns for an additional year of educacióning for women: -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!--  \mathop{\boldsymbol{E}}\left[ \text{salario}_i | \text{Female} \land \text{educación} = \ell + 1 \right] - -->
<!--     \mathop{\boldsymbol{E}}\left[ \text{salario}_i | \text{Female} \land \text{educación} = \ell \right] &= \\ -->
<!--  \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 (\ell+1) + \beta_2 + \beta_3 (\ell + 1) + u_i \right] - -->
<!--     \mathop{\boldsymbol{E}}\left[ \beta_0 + \beta_1 \ell + \beta_2 + \beta_3 \ell + u_i  \right] &= \\ -->
<!--  \beta_1 + \beta_3 -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- -- -->

<!-- Similarly, $\beta_1$ gives the expected return to an additional year of educacióning for men. Thus, $\beta_3$ gives the **difference in the returns to educacióning** for women and men. -->

---
## Especificación log-lineal

En economía, con frecuencia se emplean variables explicativas en logaritmos, por ejemplo,

$$ \log(\text{salario}_i) = \beta_0 + \beta_1 \, \text{educación}_i + u_i $$


Esta especificación cambia nuestra interpretación de los coeficientes de pendiente.

**Interpretación**

- Un aumento de una unidad en nuestra variable explicativa aumenta la variable de resultado en aproximadamente $\beta_1$ veces $100$ por ciento.

- *Ejemplo:* Un año adicional de educación aumenta el salario en aproximadamente un 3 por ciento (para $\beta_1 = 0.03$).

<!-- --- -->
<!-- # Interpretando coeficientes -->
<!-- ## Especificación log-lineal -->

<!-- **Derivación** -->

<!-- Considerando el modelo log-lineal: -->

<!-- $$ \log(y) = \beta_0 + \beta_1 \, x + u $$ -->

<!-- y diferenciando -->

<!-- $$ \dfrac{dy}{y} = \beta_1 dx $$ -->

<!-- Entonces un cambio marginal en $x$ , $\text{dx}$ , lleva a un aumento $\beta_1 dx$ **cambio porcentual** en $y$. -->

---
## Especificación log-lineal

```{R, log linear plot, echo = F, cache = T, dev = "svg", fig.height = 6}
# Set seed
set.seed(1234)
# Sample size
n <- 1e3
# Generate data
ll_df <- tibble(
  x = runif(n, 0, 3),
  y = exp(-100 + 0.75 * x + rnorm(n, sd = 0.5))
)
# Plot
ggplot(data = ll_df, aes(x = x, y = y)) +
geom_hline(yintercept = 0) +
geom_vline(xintercept = 0) +
geom_point(size = 3, color = "darkslategrey", alpha = 0.5) +
geom_smooth(color = red_pink, se = F) +
xlab("x") +
ylab("y") +
theme_axes
```

---
## Especificación log-log 

De manera similar, los econometristas emplean con frecuencia modelos log-log, en los que y se está en logaritmos y al menos una variable explicativa, también:

$$ \log(\text{salario}_i) = \beta_0 + \beta_1 \, \log(\text{educación}_i) + u_i $$

**Interpretación:**

- Un aumento del uno por ciento en $x$ dará lugar a un cambio porcentual de $\beta_1$ en $y$.
- Se interpreta como una elasticidad.

---
## Especificación log-log 

**Derivación**

Consideramos el siguiente modelo log-log:

$$ \log(y) = \beta_0 + \beta_1 \, \log(x) + u $$
y diferenciamos

$$ \dfrac{dy}{y} = \beta_1 \dfrac{dx}{x} $$

que dice que para un aumento del uno por ciento en $x$, veremos un aumento del $\beta_1$ por ciento en $y$. Como elasticidad:

$$ \dfrac{dy}{dx} \dfrac{x}{y} = \beta_1 $$

---
## Log-lineal con variable binaria

** Nota: ** Si tenemos un modelo log-lineal con una variable binaria, la interpretación del coeficiente de esa variable cambia.

Consideramos:

$$\log(y_i) = \beta_0 + \beta_1 x_1 + u_i$$

siendo $x_1$ una variable binaria.

  
La interpretación de $\beta_1$ ahora es:

- Cuando $x_1$ cambia de 0 a 1, $y$ cambiará en $100\times\left(e ^ {\beta_1} -1 \right)$ por ciento.

---
##Resumen

| Modelo      | Interpretación                                                                                            |
|-------------|-----------------------------------------------------------------------------------------------------------|
| Nivel-nivel | Incremento de unidades en "y" cuando aumenta 1 unidad la "x" (ambas en sus unidades de medida originales) |
| Log-nivel   | $\beta$*100 (incremento porcentual de "y" cuando aumenta una unidad la "x")                                      |
| Nivel-log   | $\beta$/100 (incremento en unidades de "y" cuando aumenta un 1% la "x")                                          |
| Log-log     | Incremento porcentual de "y" cuando aumenta un 1% la "x"                                                  |

