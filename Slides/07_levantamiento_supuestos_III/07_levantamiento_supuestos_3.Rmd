---
title: "Heterocedasticidad & autocorrelación"
subtitle: "Econometría I"
author: "Paula Pereda (ppereda@correo.um.edu.uy)"
date: "30 de octubre de 2021"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}

library(broom)
library(latex2exp)
library(ggplot2)
library(ggthemes)
library(viridis)
library(extrafont)
library(gridExtra)
library(magrittr)
library(knitr)
library(parallel)  
library(tibble)
  
# Define pink color
red_pink <- "#e64173"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = ">",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)
# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18, family = "STIXGeneral"),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  text = element_text(family = "MathJax_Math"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  text = element_text(family = "MathJax_Main"),
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  text = element_text(family = "Fira Sans Book"),
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
# Themes
theme_axes_y <- theme_void() + theme(
  text = element_text(family = "sans"),
  axis.title = element_text(size = 11),
  plot.title = element_text(size = 11, hjust = 0.5),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, -0.2, 0, 0, unit = "lines")),
  axis.text.y = element_text(
    size = 10, angle = 0, hjust = 0.9, vjust = 0.5,
    margin = margin(0, 0.4, 0, 0, unit = "lines")
  ),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.07, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_x <- theme_void() + theme(
  text = element_text(family = "sans"),
  axis.title = element_text(size = 11),
  plot.title = element_text(size = 11, hjust = 0.5),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, -0.2, 0, 0, unit = "lines")),
  axis.text.x = element_text(
    size = 10, angle = 0, hjust = 0.9, vjust = 0.5,
    margin = margin(0, 0.4, 0, 0, unit = "lines")
  ),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.07, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```


# Heterocedasticidad
---


Escribamos nuestros **supuestos actuales**

--

1. Nuestra muestra (los $x_k$'s y la $y_i$) fueron .hi[extraídas aleatoriamente] de la población.

--

2. $y$ es una .hi[función lineal] de los $\beta_k$'s y $u_i$.

--

3. No hay .hi[multicolinealidad perfecta] en nuestra muestra.

--

4. Las variables explicativas son .hi[exogenas]: $\mathop{\boldsymbol{E}}\left[ u \middle| X \right] = 0 \left(\implies \mathop{\boldsymbol{E}}\left[ u \right] = 0\right)$.

--

5. Los errores tiene .hi[varianza constante] $\sigma^2$ y .hi[cero covarianza], _por ejemplo_,
  - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
  - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ for $i\neq j$

--

6. Los errores provienen de una distribución .hi[Normal], _por ejemplo_, $u_i \overset{\text{iid}}{\sim} \mathop{\text{N}}\left( 0, \sigma^2 \right)$.

---

Nos vamos a enfocar en el supuesto \#5:

> 5\. Los errores tiene .hi[varianza constante] $\sigma^2$ y .hi[cero covarianza], 
> _por ejemplo_,
> - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
> - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ para $i\neq j$

--

Específicamente, nos enfocaremos en el supuesto de .hi[varianza constante] (también conocido como *homocedasticidad*).

--

**Violación del supuesto:**

.hi[Heterocedasticidad:] $\mathop{\text{Var}} \left( u_i \right) = \sigma^2_i$ y $\sigma^2_i \neq \sigma^2_j$ para algunos $i\neq j$.

--

En otras palabras: nuestros errores tienen distintas varianzas.

---

Ejemplo clásico de heterocedasticidad: el embudo

La varianza de $u$ aumenta con $x$

```{R, het ex1, dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

Otro ejemplo de heterocedasticidad: (¿doble embudo?)

Varianza de $u$ aumentando en los extremos de $x$

```{R, het ex2 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

Otra ejemplo de heterocedasticidad:

Diferentes variancias de $u$ por grupo

```{R, het ex3 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  g = sample(c(F,T), 1e3, replace = T),
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 0.5 + 2 * g)
), aes(x = x, y = e, color = g, shape = g, alpha = g)) +
geom_point(size = 2.75) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
scale_shape_manual(values = c(16, 1)) +
scale_alpha_manual(values = c(0.5, 0.8)) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

La .hi[heterocedasticidad] está presente donde la varianza de $u$ cambia con cualquier combinación de nuestras variables explicativas $x_1$, a través de $x_k$ (por lo tanto, $X$).

--

(Muy común en la práctica)

---

## Consecuencias

Entonces... ¿cuáles son las consecuencias de la heterocedasticidad? ¿Sesgo? ¿Ineficiencia?

Primero, chequeamos si tiene consecuencias para la insesgadez de MCO.

--

**Recordemos<sub>1</sub>:** MCO siendo insesgado significa $\mathop{\boldsymbol{E}}\left[ \hat{\beta}_k \middle| X \right] = \beta_k$ para todos $k$.

--

**Recordemos<sub>2</sub>:** previamente demostramos que $\hat{\beta}_1 = \dfrac{\sum_i\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2}$

--

Nos ayudará escribir el estimador como

$$ \hat{\beta}_1 = \beta_1 + \dfrac{\sum_i \left( x_i - \overline{x} \right) u_i}{\sum_i \left( x_i - \overline{x} \right)^2} $$
---

**Prueba:** Asumiendo $y_i = \beta_0 + \beta_1 x_i + u_i$

$$
\begin{aligned}
  \hat{\beta}_1
  &= \dfrac{\sum_i\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\left[ \beta_0 + \beta_1 x_i + u_i \right]- \left[ \beta_0 + \beta_1 \overline{x} + \overline{u} \right] \right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\beta_1 \left[ x_i - \overline{x} \right] + \left[u_i - \overline{u}\right]  \right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\beta_1 \left[ x_i - \overline{x} \right]^2 + \left[ x_i - \overline{x} \right] \left[u_i - \overline{u}\right]\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) \left(u_i - \overline{u}\right)}{\sum_i\left(x_i -\overline{x}\right)^2}
\end{aligned}
$$
---

$$
\begin{aligned}
  \hat{\beta}_1
  &= \cdots = \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) \left(u_i - \overline{u}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \sum_i\left(x_i - \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \left(\sum_i x_i - \sum_i \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \left(\sum_i x_i - n \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \color{#e64173}{\left(\sum_i x_i - \sum_i x_i\right)}}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \quad \text{😅}
\end{aligned}
$$
---

## Consecuencias: Sesgo

Ahora queremos saber si la heterocedasticidad sesga el estimador de MCO para $\beta_1$.

--

$$
\begin{aligned}
  \mathop{\boldsymbol{E}}\left[ \hat{\beta}_1 \middle| X \right]
  &= \mathop{\boldsymbol{E}}\left[ \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \middle| X \right] \\[0.5em]
  &= \beta_1 + \mathop{\boldsymbol{E}}\left[ \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \middle| X \right] \\[0.5em]
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \color{#e64173}{\underbrace{\mathop{\boldsymbol{E}}\left[ u_i \middle| X \right]}_{=0}} \\[0.5em]
  &= \beta_1
\end{aligned}
$$

--
.hi[MCO todavía es insesgado] para los $\beta_k$.
---

## Consecuencias: Eficiencia

La eficiencia e inferencia de MCOno sobreviven la heterocedasticidad.

- En la presencia de heterocedasticidad, MCO ya no es .hi[MELI].

--
- Sería más informativo (eficiente) .hi[ponderar las observaciones] inversamente a la varianza de su $u_i$.

  - Reducción de peso de alta variación $u_i$'s (demasiado ruidoso para aprender mucho).

  - Observaciones al alza con baja varianza $u_i$ 's (más confiable).

  - Ahora tienes la idea de mínimos cuadrados ponderados (MCP)

---

## Consecuencias: Inferencia

Los .hi[errores estándar de MCO están sesgados] en presencia de heterocedasticidad.

- Intervalos de confianza incorrectos

- Problemas para el testeo de hipotesis (tanto $t$ como $F$ tests)

--

- Es difícil aprender sin inferencia.
---

## Soluciones

1. **Tests** para determinar si hay presencia de heterocedasticidad.

2. **Remedios** para (1) la eficiencia y (2) la inferencia

---
layout: true
# Testeando la heterocedasticidad

---
class: inverse, middle

---

Si bien *podríamos* tener soluciones para la heterocedasticidad, la eficiencia de nuestros estimadores depende de si la heterocedasticidad está presente o no.

1. **Goldfeld-Quandt test**

1. **Breusch-Pagan test**

1. **White test**

--

Cada una de estas pruebas se centra en el hecho de que podemos .hi[usar el residuo de MCO] $\color{#e64173}{e_i}$ .hi[para estimar la perturbación de la población] $\color{#e64173}{u_i}$.


---
layout: true
# Testeando la heterocedasticidad
## El test de Goldfeld-Quandt
---

Se centra en un tipo específico de heterocedasticidad: si la varianza de $u_i$ difiere .hi[entre dos grupos]. <sup>†</sup>

¿Recuerda cómo usamos nuestros residuos para estimar $\sigma ^ 2$?

$$s ^ 2 = \dfrac {\text {SCE}} {n-1} = \dfrac {\sum_i e_i ^ 2} {n-1}$$

Usaremos esta misma idea para determinar si hay evidencia de que nuestros dos grupos difieren en las variaciones de sus perturbaciones, comparando efectivamente $s ^ 2_1$ y $s ^ 2_2$ de nuestros dos grupos.

.note[[†]: La prueba G-Q fue una de las primeras pruebas de heterocedasticidad (1965).]

---

Operacionalmente,

.pseudocode-small[

1. Ordenamos las observaciones por $x$

2. Dividimos los datos en dos grupos de tamaño n.super[⭑]
  - G<sub>1</sub>: El primer tercio
  - G<sub>2</sub>: El último tercio

3. Corremos regresiones separadas de $y$ en $x$ para G.sub[1] y G.sub[2]

4. Nos quedamos con SCE.sub[1] y SCE.sub[2]

5. Calculamos el estadístico del test G-Q

]
---

El estadístico del test G-Q

$$ F_{\left(n^{\star}-k,\, n^{\star}-k\right)} = \dfrac{\text{SCE}_2/(n^\star-k)}{\text{SCE}_1/(n^\star-k)} = \dfrac{\text{SCE}_2}{\text{SCE}_1} $$

sigue una distribución $F$ (bajo la hipótesis nula) con $n^{\star}-k$ y $n^{\star}-k$ grados de libertad.<sup>†</sup>

--

**Notas**

- El test G-Q requiere que los errores sigan una distribución normal.
- G-Q asuma un tipo o forma muy específica de heterocedasticidad.
- Funciona muy bien si conocemos la forma potencia de la heterocedasticidad.

.footnote[
[†]: Goldfeld and Quandt sugirieron $n^{\star}$ de $(3/8)n$. $k$ da el número de parámetros estimados (_por ejemplo_, $\hat{\beta}_j$'s).
]
---

```{R, gq1a, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq_df$x, probs = c(3/8, 5/8))
# Regressions
sse1 <- lm(y ~ x, data = subset(gq_df, x < gq_x[1])) %>%
  residuals() %>% magrittr::raise_to_power(2) %>% sum()
sse2 <- lm(y ~ x, data = subset(gq_df, x > gq_x[2])) %>%
  residuals() %>% magrittr::raise_to_power(2) %>% sum()
ggplot(data = gq_df, aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

```{R, gq1b, echo = F, dev = "svg", fig.height = 4}
ggplot(data = gq_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SCE}_2 = `r format(round(sse2, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SCE}_1 = `r format(round(sse1, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2/sse1, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-value $< 0.001$

$\therefore$ Rechazamos H.sub[0]: $\sigma^2_1 = \sigma^2_2$ y concluimos que hay evidencia estadísticamente significativa de heterocedasticidad.
---

El problema...
---

```{R, gq2, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq2_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq2_df$x, probs = c(3/8, 5/8))
# Regressions
sse1b <- lm(y ~ x, data = subset(gq2_df, x < gq_x[1])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
sse2b <- lm(y ~ x, data = subset(gq2_df, x > gq_x[2])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
ggplot(data = gq2_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SCE}_2 = `r format(round(sse2b, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SCE}_1 = `r format(round(sse1b, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2b/sse1b, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-value $\approx `r round(pf(sse2b/sse1b, 375, 375, lower.tail = F), 3)`$

$\therefore$ Fallamos en rechazar la hipótesis H.sub[0]: $\sigma^2_1 = \sigma^2_2$ cuando hay heterocedasticidad.
---
layout: true
# Testeando la heterocedasticidad
## El test de Breusch-Pagan 
---

Breusch y Pagan (1981) intentaron resolver este problema de ser muy específicos con la forma funcional de la heterocedasticidad.

- Permite a los datos mostrar cómo la varianza de $u_i$ se correlaciona con $X$.

- Si $\sigma_i^2$ se correlaciona con $X$, entonces tenemos heterocedasticidad.

- Regresa $e_i^2$ en $X = \left[ 1,\, x_1,\, x_2,\, \ldots,\, x_k \right]$ y testea la significancia conjunta.
---

Cómo se implementa:

.pseudocode-small[

1\. Regresa y en el intercepto, x.sub[1], x.sub[2], …, x.sub[k].

2\. Se queda con los residuos e.

3\. Regresa e.super[2] en el intercepto, x.sub[1], x.sub[2], …, x.sub[k].

$$e\_i^2 = \alpha\_0 + \alpha\_1 x\_{1i} + \alpha\_2 x\_{2i} + \cdots + \alpha\_k x\_{ki} + v\_i$$

4\. Se queda con R.super[2].

5\. Testea la hipótesis H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_k = 0$

]

---

El estadístico del test de B-P es

$$\text{LM} = n \times R^2_{e}$$

donde $R^2_e$ es el $R^2$ de la regresión

$$e\_i^2 = \alpha\_0 + \alpha\_1 x\_{1i} + \alpha\_2 x\_{2i} + \cdots + \alpha\_k x\_{ki} + v\_i$$

Bajo la nula, $\text{LM}$ se distribuye asintóticamente $\chi^2_k$.

--

Este estadístico testeat H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_k = 0$.

Rechazar la hipotesis nula implica que hay evidencia de heterocedasticidad.

---
layout: true
# Testeando la heterocedasticidad
## La distribución $\chi^2$

---

Acabamos de mencionar que bajo el valor nulo, el estadístico de la prueba B-P se distribuye como una variable aleatoria $\ chi^2$ con $k$ grados de libertad.

La distribución $\chi ^ 2 $ es solo otro ejemplo de una distribución común (con nombre) (como la distribución Normal, la distribución $t$ y la $F$).

---

Tres ejemplos de $\chi_k^2$: $\color{#314f4f}{k = 1}$, $\color{#e64173}{k = 2}$, y $\color{orange}{k = 9}$

```{R, chisq1, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = tibble(x = c(0, 20)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 3),
    fill = red_pink, alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 3), n = 1e3,
    color = red_pink
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 9),
    fill = "orange", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 9), n = 1e3,
    color = "orange"
  ) +
  labs(x = "x", y = "f") +
  theme_axes_math
```
---

Probabilidad de observar un estadístico más extremo $\widehat{\text{LM}}$ bajo H.sub[0]

```{R, chisq2, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = tibble(x = c(0, 8)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.05
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = red_pink, alpha = 0.85,
    xlim = c(5, 8)
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_vline(xintercept = 5, color = grey_dark, size = 0.5, linetype = "dotted") +
  annotate("text", x = 5, y = 1.55 * dchisq(5, df = 2), label = TeX("$\\widehat{LM}$"), family = "MathJax_Main", size = 7) +
  labs(x = "x", y = "f") +
  theme_axes_math
```
---
layout: true
# Testeando la heterocedasticidad
## El test de Breusch-Pagan
---

**Problema:** Seguimos asumiendo una .hi[forma funcional] bastante restrictiva entre nuestras variables explicativas $X$ y las variaciones de nuestras perturbaciones $\sigma ^ 2_i$.

--

**Resultado:** B-P *puede* aún pasar por alto formas bastante simples de heterocedasticidad.

---

Las pruebas de Breusch-Pagan siguen siendo .hi[sensibles a la forma funcional].

```{R, bp1, echo = F, dev = "svg", fig.height = 3.75}
set.seed(12345)
# Data
bp_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Regressions
lm_bp1 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
lm_bp2 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x + I(bp_df$x^2)) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
# The figure
ggplot(data = bp_df, aes(x = x, y = e)) +
geom_point(size = 2.75, color = "darkslategrey", alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

$$
\begin{aligned}
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} & \widehat{\text{LM}} &= `r round(lm_bp1, 2)` &\mathit{p}\text{-value} \approx `r round(pchisq(lm_bp1, 1, lower.tail = F), 3)` \\
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} \color{#e64173}{+ \hat{\alpha}_2 x^2_{1i}} & \widehat{\text{LM}} &= `r round(lm_bp2, 2)` &\mathit{p}\text{-value} < 0.001
\end{aligned}
$$
---
layout: true
# Testeando la heterocedasticidad
## El test de White
---

Hasta ahora hemos estado probando relaciones específicas entre nuestras variables explicativas y las varianzas de las perturbaciones, por ejemplo,

- H.sub[0]: $\sigma_1^2 = \sigma_2^2$ para dos grupos basados en $x_j$ (**G-Q**)

- H.sub[0]: $\alpha_1 = \cdots = \alpha_k = 0$ de $e_i^2 = \alpha_0 + \alpha_1 x_{1i} + \cdots + \alpha_k x_{ki} + v_i$ (**B-P**)

--

Sin embargo, en realidad queremos saber si

$$\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_n^2$$

**P:** ¿No podemos simplemente probar esta hipótesis?

--

**R:** Más o menos.
---

Con este objetivo, Hal White aprovechó el hecho de que podemos .hi[reemplazar el requisito de homocedasticidad con una suposición más débil]:

- **Viejo:** $\mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2$

- **Nuevo:** $u^2$ está *incorrelacionado* con las variables explicativas (_por ejemplo_,  $x_j$ para todo $j$), sus cuadrados (_por ejemplo_, $x_j^2$), y las interacciones d eprimer grado (_por ejemplo_, $x_j x_h$).

--

Este nuevo supuesto es más fácil de probar explícitamente (*pista:* regresión).
---

Un resumen de la prueba de White para heterocedasticidad:

.pseudocode-small[

1\. Regresamos y en x.sub[1], x.sub[2], …, x.sub[k]. Guardo los residuos e.

2\. los residuos al cuadrado de todas las variables explicativas, sus cuadrados e interacciones.

$$e^2 = \alpha\_0 + \sum\_{h=1}^k \alpha\_h x\_h + \sum\_{j=1}^k \alpha\_{k+j} x\_j^2 + \sum\_{\ell = 1}^{k-1} \sum\_{m = \ell + 1}^k \alpha\_{\ell,m} x\_\ell x\_m + v\_i$$

3\. Guardo R.sub[e].super[2].

4\. Calculo el estadístico del test para testear H.sub[0]: $\alpha_p = 0$ para todo $p\neq0$.

]
---

Al igual que con la prueba de Breusch-Pagan, la estadística de prueba de White es

$$\text{LM} = n \times R_e^2 \qquad \text{Bajo H}_0,\, \text{LM} \overset{\text{d}}{\sim} \chi_k^2$$

pero ahora $R^2_e$ proviene de la regresión de $e^2$ sobre las variables explicativas, sus cuadrados y sus interacciones.

$$e^2 = \alpha\_0 + \underbrace{\sum\_{h=1}^k \alpha\_h x\_h}\_{\text{Expl. variables}} + \underbrace{\sum\_{j=1}^k \alpha\_{k+j} x\_j^2}\_{\text{Squared terms}} + \underbrace{\sum\_{\ell = 1}^{k-1} \sum\_{m = \ell + 1}^k \alpha\_{\ell,m} x\_\ell x\_m}\_{\text{Interactions}} + v\_i$$

**Nota:** El $k$ (para nuestro $\chi_k^2$) es igual al número de parámetros estimados en la regresión de arriba (el $\alpha_j$), excluyendo el intercepto $\left( \alpha_0 \right)$.
---

**Nota práctica:** Si una variable es igual a su cuadrado (_ejemplo_, variables binarias), entonces no puede incluirla. La misma regla se aplica a las interacciones.
---

*Ejemplo:* Considere el modelo.super[†] $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$

**Paso 1:** Estime el modelo; obtenga los residuos $(e)$.

**Paso 2:** Regrese $e^2$ en las variables explicativas, sus cuadrados e interacciones.
$$
\begin{aligned}
  e^2 = \
  & \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 + \alpha_4 x_1^2 + \alpha_5 x_2^2 + \alpha_6 x_3^2 \\
  &+ \alpha_7 x_1 x_2 + \alpha_8 x_1 x_3 + \alpha_9 x_2 x_3 + v
\end{aligned}
$$

Guarde el R.super[2] de esta ecuación (llamémoslo, $R_e^2$).

**Paso 3:** Testeo H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_9 = 0$ using $\text{LM} = n R^2_e \overset{\text{d}}{\sim} \chi_9^2$.

.footnote[
[†]: Para simplificar la notación, no utilizo los subíndices $i$.
]
---

```{R, white1, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# The figure
ggplot(data = bp_df, aes(x = x, y = e)) +
geom_point(size = 2.75, color = "darkslategrey", alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

We've already done the White test for this simple linear regression.

$$
\begin{aligned}
 e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} \color{#e64173}{+ \hat{\alpha}_2 x^2_{1i}} & \widehat{\text{LM}} &= `r round(lm_bp2, 2)` &\mathit{p}\text{-value} < 0.001
\end{aligned}
$$
---
layout: true
# Heterocedasticidad
## Preguntas de repaso
---

--

- **P:** ¿Cuál es la definición de heterocedastidad?

- **P:** ¿Por qué nos preocupa la heterocedastidad?

- **P:** ¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** ¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Dado que no podemos observar $u_i$'s, ¿qué podemos usar para *aprender más* sobre la heterocedasticidad?

- **P:** ¿Qué test se recomiendo para la heterocedasticidad? ¿Por qué?

---
count: false

- **P:** ¿Cuál es la definición de heterocedastidad?
--

- **R:**
<br>.hi[Matemática:] $\mathop{\text{Var}} \left( u_i | X \right) \neq \mathop{\text{Var}} \left( u_j | X \right)$ para algún $i\neq j$.
<br>.hi[Palabras:] Hay una relación sistemática entre la varianza de $u_i$ y nuestras variables explicativas.
---
count: false

.grey-vlight[

- **P:** ¿Cuál es la definición de heterocedastidad?

]

- **P:** ¿Por qué nos preocupa la heterocedastidad?
--

- **R:** Esto sesga nuestros errores estándar, arruinando nuestras pruebas estadísticas e intervalos de confianza. Además: MCO ya no es el MELI.
---
count: false

.grey-vlight[

- **P:** ¿Cuál es la definición de heterocedastidad?

- **P:** ¿Por qué nos preocupa la heterocedastidad?

]

- **P:** ¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?
--

- **R:** No es exactamente lo que queremos, pero dado que $y$ es una función de $x$ y $u$, aún puede ser informativo. Si $y$ se vuelve más/menos disperso a medida que cambia $x$, es probable que tengamos heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** ¿Cuál es la definición de heterocedastidad?

- **P:** ¿Por qué nos preocupa la heterocedastidad?

- **P:** ¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

]

- **P:** ¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?
--

- **R:** Si. El margen de $e$ representa su varianza y nos dice algo sobre la varianza de $u$. Las tendencias en esta varianza, a lo largo de $x$, sugieren heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** ¿Cuál es la definición de heterocedastidad?

- **P:** ¿Por qué nos preocupa la heterocedastidad?

- **P:** ¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** ¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

]

- **P:** Dado que no podemos observar $u_i$'s, ¿qué podemos usar para *aprender más* sobre la heterocedasticidad?
--

- **R:** Usamos los $e_i$ para predecir/aprender sobre los $u_i$. Este truco es clave para casi todo lo que hacemos con la prueba/corrección de heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** ¿Cuál es la definición de heterocedastidad?

- **P:** ¿Por qué nos preocupa la heterocedastidad?

- **P:** ¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** ¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Dado que no podemos observar $u_i$'s, ¿qué podemos usar para *aprender más* sobre la heterocedasticidad?

]


---
layout: true
# Heterocedasticidad - ejercicio

---
class: inverse, middle
---
¿En qué subfiguras de abajo probablemente $u_i$ es heterocedástico? Expliquen brevemente. 
(***Ayuda*** Puede haber más de una.)

**Figura 1**
```{R, echo = F, dev = "svg", fig.height = 2.5}
set.seed(123)
n <- 101
# No violations
p1 <- ggplot(data = tibble(x = 1:n, u = rnorm(n)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1a") +
theme_axes_y
# Violates homoskedasticity
p2 <- ggplot(data = tibble(x = 1:n, u = rnorm(n, sd = abs(sin(x/(100))) + 0.1)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1b") +
theme_axes_y
# Violates both
p3 <- ggplot(data = tibble(x = 1:n, u = runif(n, min = -250, max = (x-50.5)^2)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1c") +
theme_axes_y
# Put it all together
grid.arrange(p1, p2, p3, nrow = 1)
```

---


.pink[
**Respuesta** $u_i$ es probablemente heterocedástico en las subfiguras **1b** y **1c**. Podemos ver tendencias claras (relaciones) entre la varianza de $u_i$ (su dispersión) y $x_i$.
]

---
**1b.** En la presencia de heterocedasticidad, ¿MCO sigue siendo insesgado?

--
.pink[
**Respuesta** Sí.
]


**1c.** ¿Qué problemas causa la heterocedasticidad en nuestro setting de MCO?
--

.pink[
**Respuesta** La heterocedasticidad hace a (1) MCO ineficiente y (2) sesga la estimación de los errores estándar.
]

**1d.** Imaginemos que queremos estimar por MCO este modelo

$$
\begin{align}
  y_i = \beta_0 + \beta_1 x_i + u_i \tag{1}
\end{align}
$$

donde $x_i$ es una variable categórica que toma los valores $1$, $2$, o $3$.

Supongamos que sabemos que $\mathop{\text{Var}} \left( u_i \middle| x_i = 1 \right) = 15$ y $\mathop{\text{Var}} \left( u_i \middle| x_i = 2 \right) = 15$. No conocemos $\mathop{\text{Var}} \left( u_i \middle| x_i = 3 \right)$, _i.e._, $\mathop{\text{Var}} \left( u_i | x_i = 3 \right) = \sigma_3^2$ para algún parámetro desconocido $\sigma_3^2$.

¿Qué valor debería tomar $\sigma_3^2$ para que nuestro modelo sea homocedástico?

--

.pink[
**Respuesta** Para que nuestro modelo sea homocedástico, $\sigma_3^2 = 15$.
]
---

**1e.** *Goldfeld-Quandt* Para probar si los datos que usaremos para estimar la ecuación $(1)$ son homocedásticos / heteroscedásticos, haremos una prueba de Goldfeld-Quandt. 

Estimamos $(1)$ para el tercio superior del conjunto de datos (ordenados en $x$) y encontramos SSE.sub [3] = 100. Estimamos $(1)$ en el tercio medio y encontramos SSE.sub [2] = 80. Finalmente, estimamos $(1)$ en el tercio inferior y encontramos SSE.sub [1] = 70. Cada uno de estos tres grupos tiene 100 observaciones. Realice una prueba de Goldfeld-Quandt. Exprese sus hipótesis, calcule el estadístico de la prueba G-Q, determine el valor *p*, concluya.

***Sugerencia:*** La función `pf(q, df1, df2, lower.tail = F)` calcula la probabilidad de observar un valor de `q` o uno mayor en una distribución $F$ con `df1, df2` grados de libertad numerador y denominador.

--

.pink[
**Respuesta** La hipotesis para nuestro test es

.b[H.sub[o]]: $\sigma_1^2 = \sigma_3^2$ (homocedasticidad) *vs.* .b[H.sub[a]]: $\sigma_1^2 \neq \sigma_3^2$ (heterocedasticidad)

Para la prueba de Goldfeld-Quandt, probamos esta hipótesis nula utilizando el estadístico de prueba

$$
\begin{align}
   F = \dfrac{SSE_3}{SSE_1} = \dfrac{100}{70} \approx 1.4286
\end{align}
$$

Bajo la hipótesis nula, este estadístico de prueba tiene una distribución $F$ con 98 (= 100-2) grados de libertad en el numerador y denominador. Usando .mono[R] podemos calcular el *p*-valor:

```{R, key-2f}
# p-value
pf(100/70, df1 = 100-2, df2 = 100-2, lower.tail = F)
```

Este valor *p* es menor que 0.05, por lo que rechazamos la hipótesis nula y concluimos que hay evidencia estadísticamente significativa de heterocedasticidad (al nivel del 5 por ciento).
]
---
layout: true
# Living with heteroskedasticity
---
class: inverse, middle, true
---

In the presence of heteroskedasticity, OLS is

- still .hi[unbiased]
- .hi[no longer the most efficient] unbiased linear estimator

On average, we get the right answer but with more noise (less precision).
<br> *Also:* Our standard errors are biased.

--

**Options:**

1. Check regression .hi[specification].
2. Find a new, more efficient .hi[unbiased estimator] for $\beta_j$'s.
3. Live with OLS's inefficiency; find a .hi[new variance estimator].
  - Standard errors
  - Confidence intervals
  - Hypothesis tests
---
layout: true
# Living with heteroskedasticity
## Misspecification
---

As we've discussed, the specification.pink[<sup>†</sup>] of your regression model matters a lot for the unbiasedness and efficiency of your estimator.

**Response \#1:** Ensure your specification doesn't cause heteroskedasticity.

.footnote[.pink[†] *Specification:* Functional form and included variables.]
---

*Example:* Let the population relationship be $$ y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i $$

with $\mathop{\boldsymbol{E}}\left[ u_i \middle| x_i \right] = 0$ and $\mathop{\text{Var}} \left( u_i \middle| x_i \right) = \sigma^2$.

However, we omit $x^2$ and estimate $$ y_i = \gamma_0 + \gamma_1 x_i + w_i $$

Then $$ w_i = u_i + \beta_2 x_i^2 \implies \mathop{\text{Var}} \left( w_i \right) = f(x_i) $$

_I.e._, the variance of $w_i$ changes systematically with $x_i$ (heteroskedasticity).
---

```{R, spec data, include = F}
# Set the seed
set.seed(1234)
# Generate data
spec_df <- tibble(x = runif(1e3, 0, 3), y = exp(0.5 + 0.6 * x + rnorm(1e3, sd = 0.3)))
# Add residuals ('w': wrong specification; 'c': correct specification)
spec_df %<>% dplyr::mutate(
  e_w = lm(y ~ x, data = spec_df) %>% residuals(),
  e_c = lm(log(y) ~ x, data = spec_df) %>% residuals()
)
```

.pink[Truth:] $\color{#e64173}{\log\left(y_i\right) = \beta_0 + \beta_1 x_i + u_i}$  .slate[**Misspecification:**] $\color{#314f4f}{y_i = \beta_0 + \beta_1 x_i + v_i}$

```{R, spec plot1, echo = F, dev = "svg", fig.height = 5}
ggplot(data = spec_df, aes(x = x)) +
  geom_point(aes(y = e_w), color = "darkslategrey", size = 2.75, alpha = 0.5, shape = 16) +
  geom_point(aes(y = e_c), color = red_pink, size = 2.5, alpha = 0, shape = 19) +
  labs(x = "x", y = "e") +
  theme_axes_math
```
---

.pink[**Truth:**] $\color{#e64173}{\log\left(y_i\right) = \beta_0 + \beta_1 x_i + u_i}$  .slate[Misspecification:] $\color{#314f4f}{y_i = \beta_0 + \beta_1 x_i + v_i}$

```{R, spec plot2, echo = F, dev = "svg", fig.height = 5}
ggplot(data = spec_df, aes(x = x)) +
  geom_point(aes(y = e_w), color = "darkslategrey", size = 2.75, alpha = 0.25, shape = 1) +
  geom_point(aes(y = e_c), color = red_pink, size = 2.5, alpha = 0.5, shape = 19) +
  labs(x = "x", y = "e") +
  theme_axes_math
```
---

More generally:

**Misspecification problem:** Incorrect specification of the regression model can cause heteroskedasticity (among other problems).

--

**Solution:** 💡 Get it right (_e.g._, don't omit $x^2$).

--

**New problems:**

- We often don't know the *right* specification.
- We'd like a more formal process for addressing heteroskedasticity.

--

**Conclusion:** Specification often will not "solve" heteroskedasticity.
<br> However, correctly specifying your model is still really important.

---
layout: true
# Living with heteroskedasticity
## Weighted least squares
---

Weighted least squares (WLS) presents another approach.

**Response \#2:** Increase efficiency by weighting our observations.

--

Let the true population relationship be

$$
\begin{align}
  y_i = \beta_0 + \beta_1 x_{i} + u_i \tag{1}
\end{align}
$$

with $u_i \sim \mathop{N} \left( 0,\, \sigma_i^2 \right)$.

--

Now transform $(1)$ by dividing each observation's data by $\sigma_i$, _i.e._,

$$
\begin{align}
  \dfrac{y_i}{\sigma_i} &= \beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{x_{i}}{\sigma_i} + \dfrac{u_i}{\sigma_i} \tag{2}
\end{align}
$$

---

$$
\begin{align}
  y_i &= \beta_0 + \beta_1 x_{i} + u_i \tag{1} \\[1em]
  \dfrac{y_i}{\sigma_i} &= \beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{x_{i}}{\sigma_i} + \dfrac{u_i}{\sigma_i} \tag{2}
\end{align}
$$

Whereas $(1)$ is heteroskedastic,
--
 $\color{#e64173}{(2)}$ .hi[is homoskedastic].

∴ OLS is efficient and unbiased for estimating the $\beta_k$ in $(2)$!

--

Why is $(2)$ homoskedastic?

--

$\mathop{\text{Var}} \left( \dfrac{u_i}{\sigma_i} \middle| x_i \right) =$
--
 $\dfrac{1}{\sigma_i^2} \mathop{\text{Var}} \left( u_i \middle| x_i \right) =$
--
 $\dfrac{1}{\sigma_i^2} \sigma_i^2 =$
--
 $1$
---

WLS is great, but we need to know $\sigma_i^2$, which is generally unlikely.

We can *slightly* relax this requirement—instead requiring

1. $\mathop{\text{Var}} \left( u_i | x_i \right) = \sigma_i^2 = \sigma^2 h(x_i)$

2. We know $h(x)$.

--

As before, we transform our heteroskedastic model into a homoskedastic model. This time we divide each observation's data<sup>.pink[†]</sup> by $\sqrt{h(x_i)}$.

.footnote[
.pink[†] Divide *all* of the data by $\sqrt{h(x_i)}$, including the intercept.
]
---

$$
\begin{align}
  y_i &= \beta_0 + \beta_1 x_{i} + u_i \tag{1} \\[1em]
  \dfrac{y_i}{\sqrt{h(x_i)}} &= \beta_0 \dfrac{1}{\sqrt{h(x_i)}} + \beta_1 \dfrac{x_{i}}{\sqrt{h(x_i)}} + \dfrac{u_i}{\sqrt{h(x_i)}} \tag{2}
\end{align}
$$
with $\mathop{\text{Var}} \left( u_i | x_i \right) = \sigma^2 h(x_i)$.

--

Now let's check that $(2)$ is indeed homoskedastic.

$\mathop{\text{Var}} \left( \dfrac{u_i}{\sqrt{h(x_i)}} \middle| x_i \right) =$
--
 $\dfrac{1}{h(x_i)} \mathop{\text{Var}} \left( u_i \middle| x_i \right) =$
--
 $\dfrac{1}{h(x_i)} \sigma^2 h(x_i) =$
--
 $\color{#e64173}{\sigma^2}$

.hi[Homoskedasticity!]
---

.hi[Weighted least squares] (WLS) estimators are a special class of .hi[generalized least squares] (GLS) estimators focused on heteroskedasticity.

--

$$
  y\_i = \beta\_0 + \beta\_1 x\_{1i} + u\_i \quad \color{#e64173}{\text{ vs. }} \quad
  \dfrac{y\_i}{\sigma\_i} = \beta\_0 \dfrac{1}{\sigma\_i} + \beta\_1 \dfrac{x\_{1i}}{\sigma\_i} + \dfrac{u\_i}{\sigma\_i}
$$

*Notes:*

1. WLS **transforms** a heteroskedastic model into a homoskedastic model.
2. **Weighting:** WLS downweights observations with higher variance $u_i$'s.
3. **Big requirement:** WLS requires that we *know* $\sigma_i^2$ for each observation.
4. WLS is generally **infeasible**. *Feasible* GLS (FGLS) offers a solution.
5. Under its assumptions: WLS is the **best linear unbiased estimator**.
---
layout: true
# Living with heteroskedasticity
## Heteroskedasticity-robust standard errors
---

**Response \#3:**

- Ignore OLS's inefficiency (in the presence of heteroskedasticity).
- Focus on .hi[unbiased estimates for our standard errors].
- In the process: Correct inference.

--

**Q:** What is a standard error?
--
<br>**A:** The .hi[standard deviation of an estimator's distribution].

--

Estimators (like $\hat{\beta}_1$) are random variables, so they have distributions.

Standard errors give us a sense of how much variability is in our estimator.

---

*Recall*: We can write the OLS estimator for $\beta_1$ as

$$ \hat{\beta}\_1 = \beta\_1 + \dfrac{\sum_i \left( x\_i - \overline{x} \right) u\_i}{\sum\_i \left( x\_i - \overline{x} \right)^2} = \beta\_1 + \dfrac{\sum_i \left( x\_i - \overline{x} \right) u\_i}{\text{SST}\_x} \tag{3} $$

--

Let $\mathop{\text{Var}} \left( u_i \middle| x_i \right) = \sigma_i^2$.

--

We can use $(3)$ to write the variance of $\hat{\beta}_1$, _i.e._,

$$ \mathop{\text{Var}} \left( \hat{\beta}_1 \middle| x_i \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 \sigma_i^2}{\text{SST}_x^2} \tag{4} $$
---

If we want unbiased estimates for our standard errors, we need an unbiased estimate for

$$ \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 \sigma_i^2}{\text{SST}_x^2} $$

Our old friend Hal White provided such an estimator:.pink[<sup>†</sup>]

$$ \widehat{\mathop{\text{Var}}} \left( \hat{\beta}_1 \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 e_i^2}{\text{SST}_x^2} $$

where the $e_i$ comes from the OLS regression of interest.

.footnote[
.pink[†] This specific equation is for simple linear regression.
]
---

Our heteroskedasticity-robust estimators for the standard error of $\beta_j$.

.hi[Case 1] Simple linear regression, $y_i = \beta_0 + \beta_1 x_i + u_i$

$$ \widehat{\mathop{\text{Var}}} \left( \hat{\beta}_1 \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 e_i^2}{\text{SST}_x^2} $$

.hi[Case 2] Multiple (linear) regression, $y_i = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_k x_{ki} + u_i$

$$ \widehat{\mathop{\text{Var}}} \left( \hat{\beta}\_j \right) = \dfrac{\sum\_i \hat{r}\_{ij}^2 e\_i^2}{\text{SST}\_{x\_j^2}} $$

where $\hat{r}_{ij}$ denotes the i.super[th] residual from regressing $x_j$ on all other explanatory variables.
---

With these standard errors, we can return to correct statistical inferencel

_E.g._, we can update our previous $t$ statistic formula with our new heteroskedasticity-robust standard erros.

$$ t = \dfrac{\text{Estimate} - \text{Hypothesized value}}{\text{Standard error}} $$
---

**Notes**

- We are still using .hi[OLS estimates for] $\color{#e64173}{\beta_j}$
- Our het.-robust standard errors use a .hi[different estimator].
- Homoskedasticity
  - Plain OLS variance estimator is more efficient.
  - Het.-robust is still unbiased.
- Heteroskedasticity
  - Plain OLS variance estimator is biased.
  - Het.-robust variance estimator is unbiased.
---

These standard errors go by many names

- Heteroskedasticity-robust standard errors
- Het.-robust standard errors
- White standard errors
- Eicker-White standard errors
- Huber standard errors
- Eicker-Huber-White standards errors
- (some other combination of Eicker, Huber, and White)

**Do not say:** "Robust standard errors". The problem: "robust" to what?
---
layout: false
class: inverse, middle

# Living with heteroskedasticity
## Examples
---
layout: true
# Living with heteroskedasticity
---

## Examples

Back to our test-scores dataset…

```{R, ex test data}
# Load packages
library(pacman)
p_load(tidyverse, Ecdat)
# Select and rename desired variables; assign to new dataset; format as tibble
test_df <- Caschool %>% select(
  test_score = testscr, ratio = str, income = avginc, enrollment = enrltot
) %>% as_tibble()
# View first 2 rows of the dataset
head(test_df, 2)
```
---
layout: true
# Living with heteroskedasticity
## Example: Model specification
---

We found significant evidence of heteroskedasticity.

Let's check if it was due to misspecifying our model.
---

Model.sub[1]: $\text{Score}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i$
<br>`lm(test_score ~ ratio + income, data = test_df)`


```{R, ex spec1, echo = F, dev = "svg", fig.height = 4.75}
# Model 1: test ~ ratio + income
test_df %<>% mutate(e1 = lm(test_score ~ ratio + income, data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income, y = e1)) +
geom_point(size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Income", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Model.sub[2]: $\log\left(\text{Score}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i$
<br>`lm(log(test_score) ~ ratio + income, data = test_df)`


```{R, ex spec2, echo = F, dev = "svg", fig.height = 4.75}
# Model 1: test ~ ratio + income
test_df %<>% mutate(e2 = lm(log(test_score) ~ ratio + income, data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income)) +
geom_point(aes(y = e2), size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Income", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Model.sub[3]: $\log\left(\text{Score}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \log\left(\text{Income}_i\right) + u_i$
<br>`lm(log(test_score) ~ ratio + log(income), data = test_df)`


```{R, ex spec3, echo = F, dev = "svg", fig.height = 4.75}
# Model 1: test ~ ratio + income
test_df %<>% mutate(e3 = lm(log(test_score) ~ ratio + log(income), data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income)) +
geom_point(aes(y = e3), size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Income", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Let's test this new specification with the White test for heteroskedasticity.

.center[
Model.sub[3]: $\log\left(\text{Score}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \log\left(\text{Income}_i\right) + u_i$
]

```{R, ex spec test, include = F}
white_r2_spec <- lm(e3^2 ~
  ratio * log(income) + I(ratio^2) + I(log(income)^2),
  data = test_df
) %>% summary() %$% r.squared
white_stat_spec <- white_r2_spec %>% multiply_by(420)
```

--
The regression for the White test
--
$$
\begin{align}
  e_i^2 = &\alpha_0 + \alpha_1 \text{Ratio}_i + \alpha_2 \log\left(\text{Income}_i\right) + \alpha_3 \text{Ratio}_i^2 + \alpha_4 \left(\log\left(\text{Income}_i\right)\right)^2 \\
  &+ \alpha_5 \left(\text{Ratio}_i\times\log\left(\text{Income}_i\right)\right) + v_i
\end{align}
$$
--
yields $R_e^2\approx`r round(white_r2_spec, 3)`$
--
 and test statistic of
--
 $\widehat{\text{LM}} = n\times R_e^2 \approx `r round(white_stat_spec, 1)`$.

--

Under H.sub[0], $\text{LM}$ is distributed as
--
 $\chi_5^2$
--
 $\implies$ *p*-value $\approx$ `r pchisq(white_stat_spec, 5, lower.tail = F) %>% round(3)`.

--

∴
--
 .hi[Reject H.sub[0].]
--
 .hi[Conclusion:]
--
 There is statistically significant evidence of heteroskedasticity at the five-percent level.
---

Okay, we tried adjusting our specification, but there is still evidence of heteroskedasticity.

**Next:** In general, you will turn to heteroskedasticity-robust standard errors.

- OLS is still unbiased for the .hi[coefficients] (the $\beta_j$'s)

- Heteroskedasticity-robust standard errors are unbiased for the .hi[standard errors] of the $\hat{\beta}_j$'s, _i.e._, $\sqrt{\mathop{\text{Var}} \left( \hat{\beta}_j \right)}$.
---
layout: true
# Living with heteroskedasticity
## Example: Het.-robust standard errors
---

Let's return to our model

$$ \text{Score}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i $$

We can use the `lfe` package in .mono[R] to calculate standard errors.
---

$$ \text{Score}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i $$

1\. Run the regression with `felm()` (instead of `lm()`)
```{R, lfe1}
# Load 'lfe' package
p_load(lfe)
# Regress log score on ratio and log income
test_reg <- felm(test_score ~ ratio + income, data = test_df)
```

--

*Notice* that `felm()` uses the same syntax as `lm()` for this regression.
---

$$ \text{Score}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i $$

2\. Estimate het.-robust standard errors with `robust = T` option in `summary()`
```{R, lfe2a, eval = F}
# Het-robust standard errors with 'robust = T'
summary(test_reg, robust = T)
```
```{R, lfe2b, echo = F}
test_het_out <- summary(test_reg, robust = T) %>% capture.output()
test_het_out[10:13] %>% paste0("\n") %>% cat()
```
---

Ceofficients and **heteroskedasticity-robust standard errors**:
```{R, lfe3, eval = F}
summary(test_reg, robust = T)
```
```{R, lfe4, echo = F}
test_het_out <- summary(test_reg, robust = T) %>% capture.output()
test_het_out[10:13] %>% paste0("\n") %>% cat()
```

Ceofficients and **plain OLS standard errors** (assumes homoskedasticity):
```{R, lfe5, eval = F}
summary(test_reg, robust = F)
```
```{R, lfe6, echo = F}
test_hom_out <- summary(test_reg, robust = F) %>% capture.output()
test_hom_out[10:13] %>% paste0("\n") %>% cat()
```
---
layout: true
# Living with heteroskedasticity
## Example: WLS
---

We mentioned that WLS is often not possible—we need to know the functional form of the heteroskedasticity—either

**A**\. $\sigma_i^2$

or

**B**\. $h(x_i)$, where $\sigma_i^2 = \sigma^2 h(x_i)$

--

There *are* occasions in which we can know $h(x_i)$.
---

Imagine individuals in a population have homoskedastic disturbances.

However, instead of observing individuals' data, we observe (in data) groups' averages (_e.g._, cities, counties, school districts).

If these groups have different sizes, then our dataset will be heteroskedastic—in a predictable fashion.

**Recall:** The variance of the sample mean depends upon the sample size,
$$ \mathop{\text{Var}} \left( \overline{x} \right) = \dfrac{\sigma_x^2}{n} $$

--

**Example:**  Our school testing data is averaged at the school level.
---

*Example:*  Our school testing data is averaged at the school level.

Even if individual students have homoskedastic disturbances, the schools would have heteroskedastic disturbances, _i.e._,

**Individual-level model:** $\quad \text{Score}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i$

**School-level model:** $\quad \overline{\text{Score}}_s = \beta_0 + \beta_1 \overline{\text{Ratio}}_s + \beta_2 \overline{\text{Income}}_s + \overline{u}_s$

where the $s$ subscript denotes an individual school (just as $i$ indexes an individual person).

$$ \mathop{\text{Var}} \left( \overline{u}_s \right) = \dfrac{\sigma^2}{n_s} $$
---

For WLS, we're looking for a function $h(x_s)$ such that $\mathop{\text{Var}} \left( \overline{u}_s | x_s \right) = \sigma^2 h(x_s)$.

--

We just showed<sup>.pink[†]</sup> that $\mathop{\text{Var}} \left( \overline{u}_s |x_s \right) = \dfrac{\sigma^2}{n_s}$.

.footnote[
.pink[†] Assuming the individuals' disturbances are homoskedastic.
]

--

Thus, $h(x_s) = 1/n_s$, where $n_s$ is the number of students in school $s$.

--

To implement WLS, we divide each observation's data by $1/\sqrt{h(x_s)}$, meaning we need to multiply each school's data by $\sqrt{n_s}$.

--

The variable .mono[enrollment] in the .mono[test_df] dataset is our $n_s$.
---

Using WLS to estimate $\text{Score}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i$

**Step 1:** Multiply each variable by $1/\sqrt{h(x_i)} = \sqrt{\text{Enrollment}_i}$

```{R, wls1}
# Create WLS transformed variables, multiplying by sqrt of 'pop'
test_df <- mutate(test_df,
  test_score_wls = test_score * sqrt(enrollment),
  ratio_wls      = ratio * sqrt(enrollment),
  income_wls     = income * sqrt(enrollment),
  intercept_wls  = 1 * sqrt(enrollment)
)
```

Notice that we are creating a transformed intercept.
---

Using WLS to estimate $\text{Score}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Income}_i + u_i$

**Step 2:** Run our WLS (transformed) regression

```{R, wls2}
# WLS regression
wls_reg <- lm(
  test_score_wls ~ -1 + intercept_wls + ratio_wls + income_wls,
  data = test_df
)
```
--
*Note:* The `-1` in our regression tells .mono[R] not to add an intercept, since we are adding a transformed intercept (`intercept_wls`).
---

The .hi[WLS estimates and standard errors:]
```{R, wls3, echo = F}
# Grab the summary
test_wls_out <- summary(wls_reg) %>% capture.output()
# Print the coefficients
test_wls_out[11:14] %>% paste0("\n") %>% cat()
```
--
<br>
The .hi[OLS estimates] and .hi[het.-robust standard errors:]
```{R, wls4, echo = F}
# Print the coefficients
test_het_out[10:13] %>% paste0("\n") %>% cat()
```
---

Alternative to doing your own weighting: feed `lm()` some `weights`.

```{R, wls5, eval = F}
lm(test_score ~ ratio + income, data = test_df, weights = enrollment)
```
---
layout: false

# Living with heteroskedasticity

In this example

- .hi[Heteroskedasticity-robust standard errors] did not change our standard errors very much (relative to plain OLS standard errors).

- .hi[WLS] changed our answers a bit—coefficients and standard errors.

--

These examples highlighted a few things:

1. Using the correct estimator for your standard errors really matters.<sup>.pink[†]</sup>

2. Econometrics doesn't always offer an obviously *correct* route.

.footnote[
.pink[†] Sit in on an economics seminar, and you will see what I mean.
]

--

To see \#1, let's run a simulation.
---
layout: true
# Living with heteroskedasticity
## Simulation
---

```{R, sim params, include = F}
b0 <- 1L
b1 <- 10L
```


Let's examine a simple linear regression model with heteroskedasticity.

$$ y\_i = \underbrace{\beta\_0}\_{=`r b0`} + \underbrace{\beta\_1}\_{=`r b1`} x\_i + u\_i $$

where $\mathop{\text{Var}} \left( u_i | x_i \right) = \sigma_i^2 = \sigma^2 x_i^2$.
--
```{R, sim plot y, echo = F, fig.height = 4}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, 0.5, 1.5),
  y = b0 + b1 * x + rnorm(1e3, 0, sd = x^2)
), aes(x, y)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
geom_abline(intercept = b0, slope = b1, color = "orange", size = 1.5, alpha = 0.85) +
theme_axes_math
```
---

Let's examine a simple linear regression model with heteroskedasticity.

$$ y\_i = \underbrace{\beta\_0}\_{=`r b0`} + \underbrace{\beta\_1}\_{=`r b1`} x\_i + u\_i $$

where $\mathop{\text{Var}} \left( u_i | x_i \right) = \sigma_i^2 = \sigma^2 x_i^2$.
```{R, sim plot u, echo = F, fig.height = 4}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, 0.5, 1.5),
  u = rnorm(1e3, 0, sd = x^2)
), aes(x = x, y = u)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

*Note regarding WLS:*

Since $\mathop{\text{Var}} \left( u_i | x_i \right) = \sigma^2 x_i^2$,

$$ \mathop{\text{Var}} \left( u_i | x_i \right) = \sigma^2 h(x_i) \implies h(x_i) = x_i^2 $$

WLS multiplies each variable by $1/\sqrt{h(x_i)} = 1/x_i$.
---

In this simulation, we want to compare

1. The **efficiency** of
  - OLS
  - WLS with correct weights: $h(x_i) = x_i$
  - WLS with incorrect weights: $h(x_i) = \sqrt{x_i}$

2. How well our **standard errors** perform (via confidence intervals) with
  - Plain OLS standard errors
  - Heteroskedasticity-robust standard errors
  - WLS standard errors
---

The simulation plan:

.pseudocode-small[

Do 10,000 times:

1. Generate a sample of size 30 from the population

2. Calculate/save OLS and WLS (×2) estimates for β.sub[1]

3. Calculate/save standard errors for β.sub[1] using
  - Plain OLS standard errors
  - Heteroskedasticity-robust standard errors
  - WLS (correct)
  - WLS (incorrect)

]
---

**For one iteration of the simulation:**

Code to generate the data...

```{R, sim one iteration 1, eval = F}
# Parameters
b0 <- 1
b1 <- 10
s2 <- 1
# Sample size
n <- 30
# Generate data
sample_df <- tibble(
  x = runif(n, 0.5, 1.5),
  y = b0 + b1 * x + rnorm(n, 0, sd = s2 * x^2)
)
```
---

**For one iteration of the simulation:**

Code to estimate our coefficients and standard errors...

```{R, sim one iteration 2, eval = F}
# OLS
ols <- felm(y ~ x, data = sample_df)
# WLS: Correct weights
wls_t <- lm(y ~ x, data = sample_df, weights = 1/x^2)
# WLS: Correct weights
wls_f <- lm(y ~ x, data = sample_df, weights = 1/x)
# Coefficients and standard errors
summary(ols, robust = F)
summary(ols, robust = T)
summary(wls_t)
summary(wls_f)
```

Then save the results.
---
layout: false
# Living with heteroskedasticity
## Simulation: Coefficients

```{R, sim df, include = F, cache = T}
# Parameters
b0 <- 1
b1 <- 10
s2 <- 1
# Sample size
n <- 30
# Number of iterations
n_iter <- 1e4
# Set seed
set.seed(1234)
# The simulation
sim_df <- mclapply(X = 1:n_iter, FUN = function(i, size) {
  # Generate data
  sample_df <- tibble(
    x = runif(size, 0.5, 1.5),
    y = b0 + b1 * x + rnorm(size, 0, sd = s2 * x^2)
  )
  # OLS
  ols <- felm(y ~ x, data = sample_df)
  # WLS: Correct weights
  wls_t <- lm(y ~ x, data = sample_df, weights = 1/x^2)
  # WLS: Correct weights
  wls_f <- lm(y ~ x, data = sample_df, weights = 1/x)
  # Save results
  iter_df <- rbind(
    summary(ols, robust = F) %>% coef() %>% magrittr::extract(2,1:2),
    summary(ols, robust = T) %>% coef() %>% magrittr::extract(2,1:2),
    summary(wls_t) %>% coef() %>% magrittr::extract(2,1:2),
    summary(wls_f) %>% coef() %>% magrittr::extract(2,1:2)
  ) %>%
  as_tibble() %>%
  mutate(
    model = c("OLS Hom.", "OLS Het.", "WLS T", "WLS F"),
    iter = i
  )
  # Return the data
  return(iter_df)
}, mc.cores = 3, size = n) %>% bind_rows()
# Change names
names(sim_df) <- c("coef", "se", "model", "iter")
```

```{R, sim plot efficiency, echo = F, fig.height = 6.25}
ggplot(data = sim_df %>% filter(model != "OLS Hom."), aes(x = coef, color = model, fill = model)) +
geom_vline(xintercept = 10, linetype = "dashed") +
geom_density(alpha = 0.1) +
geom_hline(yintercept = 0) +
labs(x = "Estimated coefficient", y = "Density") +
scale_color_viridis_d("",
  labels = c("OLS", "WLS Incorrect", "WLS Correct"),
  end = 0.9, option = "C"
) +
scale_fill_viridis_d("",
  labels = c("OLS", "WLS Incorrect", "WLS Correct"),
  end = 0.9, option = "C"
) +
theme_pander(base_size = 22, base_family = "Fira Sans") +
theme(
  legend.position = c(.85,.9),
  # legend.background = element_blank(),
  legend.key.size = unit(1, "cm")
)
```
---
layout: false
# Living with heteroskedasticity
## Simulation: Inference

```{R, sim plot t stat, echo = F, fig.height = 6.25}
ggplot(data = sim_df, aes(x = (coef-10)/se, color = model, fill = model)) +
geom_vline(xintercept = qt(c(0.025, 0.975), df = 28), linetype = "dashed") +
geom_density(alpha = 0.1) +
geom_hline(yintercept = 0) +
labs(x = "t statistic testing the true value", y = "Density") +
scale_color_viridis_d("",
  labels = c("OLS + Het.-robust", "Plain OLS", "WLS Incorrect", "WLS Correct"),
  end = 0.9, option = "C"
) +
scale_fill_viridis_d("",
labels = c("OLS + Het.-robust", "Plain OLS", "WLS Incorrect", "WLS Correct"),
  end = 0.9, option = "C"
) +
theme_pander(base_size = 22, base_family = "Fira Sans") +
theme(
  legend.position = c(.85,.9),
  # legend.background = element_blank(),
  legend.key.size = unit(1, "cm")
)
```
---
layout: false
# Living with heteroskedasticity
## Simulation: Results

Summarizing our simulation results (10,000 iterations)

.pull-left[
<center>
**Estimation**: Summary of $\hat{\beta}_1$'s
</center>
```{R, sim table coef, eval = T, echo = F}
sim_df %>%
  filter(model != "OLS Hom.") %>%
  mutate(Estimator = recode(model,
    "OLS Het." = "OLS",
    "WLS F" = "WLS Incorrect",
    "WLS T" = "WLS Correct"
  )) %>%
  group_by(Estimator) %>%
  summarize(Mean = mean(coef) %>% round(3), "S.D." = sd(coef) %>% round(3)) %>%
  kable() %>%
  kable_styling(full_width = T)
```
]

--

.pull-right[
<center>
**Inference:** % of times we reject $\beta_1$
</center>
```{R, sim table se, eval = T, echo = F}
sim_df %>%
  mutate(Estimators = recode(model,
    "OLS Hom." = "OLS + Homosk.",
    "OLS Het." = "OLS + Het.-robust",
    "WLS F" = "WLS Incorrect",
    "WLS T" = "WLS Correct"
  )) %>%
  group_by(Estimators) %>%
  summarize(`% Reject` = mean(abs(coef-10)/se > qt(0.975, 28)) %>% multiply_by(100) %>% round(1)) %>%
  kable() %>%
  kable_styling(full_width = T)
```
]

---
layout: true
# Autocorrelación

---
class: inverse, middle

---
# Autocorrelación
## ¿Qué es?

La .hi[autocorrelación] ocurre cuando nuestras perturbaciones están correlacionadas en el tiempo, _es decir_, $\mathop{\text{Cov}} \left( u_t,\, u_s \right) \neq 0$ para $t\neq s$.

--

Otra forma de pensar: si el *shock* de la perturbación $t$ se correlaciona con los shocks "cercanos" en $t-1$ y $t + 1$.

--

*Nota:* La **correlación serial** y la **autocorrelación** son lo mismo.

---
layout: false
# Autocorrelación
## MCO

Para **modelos estáticos** o **modelos dinámicos con variables explicativas rezagadas**, en presencia de autocorrelación:

1. MCO proporciona estimaciones .pink[**insesgadas** para los coeficientes].

2. MCO crea .pink[estimaciones **sesgadas** para los errores estándar].

3. MCO es .pink[**ineficiente**.]

*Recuerden:* Las mismas implicancias que la heterocedasticidad.

