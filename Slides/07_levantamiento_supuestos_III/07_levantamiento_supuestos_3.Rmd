---
title: "(Heterocedasticidad & autocorrelaciÃ³n) + Bootstrap"
subtitle: "EconometrÃ­a I"
author: "Paula Pereda (ppereda@correo.um.edu.uy)"
date: "7 de octubre de 2021"
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---
class: inverse, middle

```{r Setup, include = F}

library(broom)
library(latex2exp)
library(ggplot2)
library(ggthemes)
library(viridis)
library(extrafont)
library(gridExtra)
library(magrittr)
library(knitr)
library(parallel)  
library(tibble)
library(extrafont)
library(kableExtra)
  
# Define pink color
red_pink <- "#e64173"
grey_light <- "grey70"
grey_mid <- "grey50"
grey_dark <- "grey20"
# Dark slate grey: #314f4f
# Knitr options
opts_chunk$set(
  comment = ">",
  fig.align = "center",
  fig.height = 7,
  fig.width = 10.5,
  warning = F,
  message = F
)

# A blank theme for ggplot
theme_empty <- theme_bw() + theme(
  line = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  plot.margin = structure(c(0, 0, -0.5, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_simple <- theme_bw() + theme(
  line = element_blank(),
  panel.grid = element_blank(),
  rect = element_blank(),
  strip.text = element_blank(),
  axis.text.x = element_text(size = 18),
  axis.text.y = element_blank(),
  axis.ticks = element_blank(),
  plot.title = element_blank(),
  axis.title = element_blank(),
  # plot.margin = structure(c(0, 0, -1, -1), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_math <- theme_void() + theme(
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes_serif <- theme_void() + theme(
  axis.title = element_text(size = 22),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = "grey70",
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
theme_axes <- theme_void() + theme(
  axis.title = element_text(size = 18),
  axis.title.x = element_text(hjust = .95, margin = margin(0.15, 0, 0, 0, unit = "lines")),
  axis.title.y = element_text(vjust = .95, margin = margin(0, 0.15, 0, 0, unit = "lines")),
  axis.line = element_line(
    color = grey_light,
    size = 0.25,
    arrow = arrow(angle = 30, length = unit(0.15, "inches")
  )),
  plot.margin = structure(c(1, 0, 1, 0), unit = "lines", valid.unit = 3L, class = "unit"),
  legend.position = "none"
)
```


# Heterocedasticidad
---


Escribamos nuestros **supuestos actuales**

--

1. Nuestra muestra (los $x_k$'s y la $y_i$) fueron .hi[extraÃ­das aleatoriamente] de la poblaciÃ³n.

--

2. $y$ es una .hi[funciÃ³n lineal] de los $\beta_k$'s y $u_i$.

--

3. No hay .hi[multicolinealidad perfecta] en nuestra muestra.

--

4. Las variables explicativas son .hi[exogenas]: $\mathop{\boldsymbol{E}}\left[ u \middle| X \right] = 0 \left(\implies \mathop{\boldsymbol{E}}\left[ u \right] = 0\right)$.

--

5. Los errores tiene .hi[varianza constante] $\sigma^2$ y .hi[cero covarianza], _por ejemplo_,
  - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
  - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ for $i\neq j$

--

6. Los errores provienen de una distribuciÃ³n .hi[Normal], _por ejemplo_, $u_i \overset{\text{iid}}{\sim} \mathop{\text{N}}\left( 0, \sigma^2 \right)$.

---

Nos vamos a enfocar en el supuesto \#5:

> 5\. Los errores tiene .hi[varianza constante] $\sigma^2$ y .hi[cero covarianza], 
> _por ejemplo_,
> - $\mathop{\boldsymbol{E}}\left[ u_i^2 \middle| X \right] = \mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2 \implies \mathop{\text{Var}} \left( u_i \right) = \sigma^2$
> - $\mathop{\text{Cov}} \left( u_i, \, u_j \middle| X \right) = \mathop{\boldsymbol{E}}\left[ u_i u_j \middle| X \right] = 0$ para $i\neq j$

--

EspecÃ­ficamente, nos enfocaremos en el supuesto de .hi[varianza constante] (tambiÃ©n conocido como *homocedasticidad*).

--

**ViolaciÃ³n del supuesto:**

.hi[Heterocedasticidad:] $\mathop{\text{Var}} \left( u_i \right) = \sigma^2_i$ y $\sigma^2_i \neq \sigma^2_j$ para algunos $i\neq j$.

--

En otras palabras: nuestros errores tienen distintas varianzas.

---

Ejemplo clÃ¡sico de heterocedasticidad: el embudo

La varianza de $u$ aumenta con $x$

```{R, het ex1, dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

Otro ejemplo de heterocedasticidad: (Â¿doble embudo?)

Varianza de $u$ aumentando en los extremos de $x$

```{R, het ex2 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2)
), aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

Otra ejemplo de heterocedasticidad:

Diferentes variancias de $u$ por grupo

```{R, het ex3 , dev = "svg", echo = F, fig.height = 5.5}
set.seed(12345)
ggplot(data = tibble(
  g = sample(c(F,T), 1e3, replace = T),
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 0.5 + 2 * g)
), aes(x = x, y = e, color = g, shape = g, alpha = g)) +
geom_point(size = 2.75) +
scale_color_manual(values = c("darkslategrey", red_pink)) +
scale_shape_manual(values = c(16, 1)) +
scale_alpha_manual(values = c(0.5, 0.8)) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

La .hi[heterocedasticidad] estÃ¡ presente donde la varianza de $u$ cambia con cualquier combinaciÃ³n de nuestras variables explicativas $x_1$, a travÃ©s de $x_k$ (por lo tanto, $X$).

--

(Muy comÃºn en la prÃ¡ctica)

---

## Consecuencias

Entonces... Â¿cuÃ¡les son las consecuencias de la heterocedasticidad? Â¿Sesgo? Â¿Ineficiencia?

Primero, chequeamos si tiene consecuencias para la insesgadez de MCO.

--

**Recordemos<sub>1</sub>:** MCO siendo insesgado significa $\mathop{\boldsymbol{E}}\left[ \hat{\beta}_k \middle| X \right] = \beta_k$ para todos $k$.

--

**Recordemos<sub>2</sub>:** previamente demostramos que $\hat{\beta}_1 = \dfrac{\sum_i\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2}$

--

Nos ayudarÃ¡ escribir el estimador como

$$ \hat{\beta}_1 = \beta_1 + \dfrac{\sum_i \left( x_i - \overline{x} \right) u_i}{\sum_i \left( x_i - \overline{x} \right)^2} $$
---

**Prueba:** Asumiendo $y_i = \beta_0 + \beta_1 x_i + u_i$

$$
\begin{aligned}
  \hat{\beta}_1
  &= \dfrac{\sum_i\left(y_i-\overline{y}\right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\left[ \beta_0 + \beta_1 x_i + u_i \right]- \left[ \beta_0 + \beta_1 \overline{x} + \overline{u} \right] \right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\beta_1 \left[ x_i - \overline{x} \right] + \left[u_i - \overline{u}\right]  \right)\left(x_i-\overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \dfrac{\sum_i\left(\beta_1 \left[ x_i - \overline{x} \right]^2 + \left[ x_i - \overline{x} \right] \left[u_i - \overline{u}\right]\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) \left(u_i - \overline{u}\right)}{\sum_i\left(x_i -\overline{x}\right)^2}
\end{aligned}
$$
---

$$
\begin{aligned}
  \hat{\beta}_1
  &= \cdots = \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) \left(u_i - \overline{u}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \sum_i\left(x_i - \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \left(\sum_i x_i - \sum_i \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \left(\sum_i x_i - n \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i - \overline{u} \color{#e64173}{\left(\sum_i x_i - \sum_i x_i\right)}}{\sum_i\left(x_i -\overline{x}\right)^2} \\
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \quad \text{ðŸ˜…}
\end{aligned}
$$
---

## Consecuencias: Sesgo

Ahora queremos saber si la heterocedasticidad sesga el estimador de MCO para $\beta_1$.

--

$$
\begin{aligned}
  \mathop{\boldsymbol{E}}\left[ \hat{\beta}_1 \middle| X \right]
  &= \mathop{\boldsymbol{E}}\left[ \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \middle| X \right] \\[0.5em]
  &= \beta_1 + \mathop{\boldsymbol{E}}\left[ \dfrac{\sum_i\left(x_i - \overline{x}\right) u_i}{\sum_i\left(x_i -\overline{x}\right)^2} \middle| X \right] \\[0.5em]
  &= \beta_1 + \dfrac{\sum_i\left(x_i - \overline{x}\right)}{\sum_i\left(x_i -\overline{x}\right)^2} \color{#e64173}{\underbrace{\mathop{\boldsymbol{E}}\left[ u_i \middle| X \right]}_{=0}} \\[0.5em]
  &= \beta_1
\end{aligned}
$$

--
.hi[MCO todavÃ­a es insesgado] para los $\beta_k$.
---

## Consecuencias: Eficiencia

La eficiencia e inferencia de MCO no sobreviven la heterocedasticidad.

- En la presencia de heterocedasticidad, MCO ya no es .hi[MELI].

--
- SerÃ­a mÃ¡s informativo (eficiente) .hi[ponderar las observaciones] inversamente a la varianza de su $u_i$.

  - ReducciÃ³n de peso de alta variaciÃ³n $u_i$'s (demasiado ruidoso para aprender mucho).

  - Observaciones al alza con baja varianza $u_i$ 's (mÃ¡s confiable).

  - Ahora tienes la idea de mÃ­nimos cuadrados ponderados (MCP)

---

## Consecuencias: Inferencia

Los .hi[errores estÃ¡ndar de MCO estÃ¡n sesgados] en presencia de heterocedasticidad.

- Intervalos de confianza incorrectos

- Problemas para el testeo de hipotesis (tanto $t$ como $F$ tests)

--

- Es difÃ­cil aprender sin inferencia.
---

## Soluciones

1. **Tests** para determinar si hay presencia de heterocedasticidad.

2. **Remedios** para (1) la eficiencia y (2) la inferencia

---
layout: true
# Testeando la heterocedasticidad

---
class: inverse, middle

---

Si bien *podrÃ­amos* tener soluciones para la heterocedasticidad, la eficiencia de nuestros estimadores depende de si la heterocedasticidad estÃ¡ presente o no.

1. **Goldfeld-Quandt test**

1. **Breusch-Pagan test**

1. **White test**

--

Cada una de estas pruebas se centra en el hecho de que podemos .hi[usar el residuo de MCO] $\color{#e64173}{e_i}$ .hi[para estimar la perturbaciÃ³n de la poblaciÃ³n] $\color{#e64173}{u_i}$.


---
layout: true
# Testeando la heterocedasticidad
## El test de Goldfeld-Quandt
---

Se centra en un tipo especÃ­fico de heterocedasticidad: si la varianza de $u_i$ difiere .hi[entre dos grupos]. <sup>â€ </sup>

Â¿Recuerda cÃ³mo usamos nuestros residuos para estimar $\sigma ^ 2$?

$$s ^ 2 = \dfrac {\text {SCE}} {n-1} = \dfrac {\sum_i e_i ^ 2} {n-1}$$

Usaremos esta misma idea para determinar si hay evidencia de que nuestros dos grupos difieren en las variaciones de sus perturbaciones, comparando efectivamente $s ^ 2_1$ y $s ^ 2_2$ de nuestros dos grupos.

.note[[â€ ]: La prueba G-Q fue una de las primeras pruebas de heterocedasticidad (1965).]

---

Operacionalmente,

.pseudocode-small[

1. Ordenamos las observaciones por $x$

2. Dividimos los datos en dos grupos de tamaÃ±o n.super[â­‘]
  - G<sub>1</sub>: El primer tercio
  - G<sub>2</sub>: El Ãºltimo tercio

3. Corremos regresiones separadas de $y$ en $x$ para G.sub[1] y G.sub[2]

4. Nos quedamos con SCE.sub[1] y SCE.sub[2]

5. Calculamos el estadÃ­stico del test G-Q

]
---

El estadÃ­stico del test G-Q

$$ F_{\left(n^{\star}-k,\, n^{\star}-k\right)} = \dfrac{\text{SCE}_2/(n^\star-k)}{\text{SCE}_1/(n^\star-k)} = \dfrac{\text{SCE}_2}{\text{SCE}_1} $$

sigue una distribuciÃ³n $F$ (bajo la hipÃ³tesis nula) con $n^{\star}-k$ y $n^{\star}-k$ grados de libertad.<sup>â€ </sup>

--

**Notas**

- El test G-Q requiere que los errores sigan una distribuciÃ³n normal.
- G-Q asuma un tipo o forma muy especÃ­fica de heterocedasticidad.
- Funciona muy bien si conocemos la forma potencia de la heterocedasticidad.

.footnote[
[â€ ]: Goldfeld y Quandt sugirieron $n^{\star}$ de $(3/8)n$. $k$ da el nÃºmero de parÃ¡metros estimados (_por ejemplo_, $\hat{\beta}_j$'s).
]
---

```{R, gq1a, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 4 + 1.5 * x),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq_df$x, probs = c(3/8, 5/8))
# Regressions
sse1 <- lm(y ~ x, data = subset(gq_df, x < gq_x[1])) %>%
  residuals() %>% magrittr::raise_to_power(2) %>% sum()
sse2 <- lm(y ~ x, data = subset(gq_df, x > gq_x[2])) %>%
  residuals() %>% magrittr::raise_to_power(2) %>% sum()
ggplot(data = gq_df, aes(x = x, y = e)) +
geom_point(color = "darkslategrey", size = 2.75, alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```
---

```{R, gq1b, echo = F, dev = "svg", fig.height = 4}
ggplot(data = gq_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SCE}_2 = `r format(round(sse2, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SCE}_1 = `r format(round(sse1, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2/sse1, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-valor $< 0.001$

$\therefore$ Rechazamos H.sub[0]: $\sigma^2_1 = \sigma^2_2$ y concluimos que hay evidencia estadÃ­sticamente significativa de heterocedasticidad.
---

El problema...
---

```{R, gq2, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# Data
gq2_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Quantiles
gq_x <- quantile(gq2_df$x, probs = c(3/8, 5/8))
# Regressions
sse1b <- lm(y ~ x, data = subset(gq2_df, x < gq_x[1])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
sse2b <- lm(y ~ x, data = subset(gq2_df, x > gq_x[2])) %>%
  residuals() %>% raise_to_power(2) %>% sum()
ggplot(data = gq2_df, aes(
  x = x, y = e,
  color = cut(x, c(-Inf, gq_x, Inf)),
  alpha = cut(x, c(-Inf, gq_x, Inf)),
  shape = cut(x, c(-Inf, gq_x, Inf))
)) +
geom_vline(
  xintercept = gq_x,
  color = grey_mid,
  size = 0.25
) +
geom_point(size = 2.75) +
labs(x = "x", y = "u") +
scale_color_manual(values = c("darkslategrey", grey_mid, red_pink)) +
scale_shape_manual(values = c(19, 1, 19)) +
scale_alpha_manual(values = c(0.5, 0.8, 0.6)) +
theme_axes_math
```

$F_{375,\,375} = \dfrac{\color{#e64173}{\text{SCE}_2 = `r format(round(sse2b, 1), nsmall = 0L, big.mark = ",")`}}{\color{#314f4f}{\text{SCE}_1 = `r format(round(sse1b, 1), nsmall = 0L, big.mark = ",")`}} \approx `r format(round(sse2b/sse1b, 1), nsmall = 0L, big.mark = ",")` \implies$ *p*-valor $\approx `r round(pf(sse2b/sse1b, 375, 375, lower.tail = F), 3)`$

$\therefore$ Fallamos en rechazar la hipÃ³tesis H.sub[0]: $\sigma^2_1 = \sigma^2_2$ cuando hay heterocedasticidad.
---
layout: true
# Testeando la heterocedasticidad
## El test de Breusch-Pagan 
---

Breusch y Pagan (1981) intentaron resolver este problema de ser muy especÃ­ficos con la forma funcional de la heterocedasticidad.

- Permite a los datos mostrar cÃ³mo la varianza de $u_i$ se correlaciona con $X$.

- Si $\sigma_i^2$ se correlaciona con $X$, entonces tenemos heterocedasticidad.

- Regresa $e_i^2$ en $X = \left[ 1,\, x_1,\, x_2,\, \ldots,\, x_k \right]$ y testea la significancia conjunta.
---

CÃ³mo se implementa:

.pseudocode-small[

1\. Regresa y en el intercepto, x.sub[1], x.sub[2], â€¦, x.sub[k].

2\. Se queda con los residuos e.

3\. Regresa e.super[2] en el intercepto, x.sub[1], x.sub[2], â€¦, x.sub[k].

$$e\_i^2 = \alpha\_0 + \alpha\_1 x\_{1i} + \alpha\_2 x\_{2i} + \cdots + \alpha\_k x\_{ki} + v\_i$$

4\. Se queda con R.super[2].

5\. Testea la hipÃ³tesis H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_k = 0$

]

---

El estadÃ­stico del test de B-P es

$$\text{LM} = n \times R^2_{e}$$

donde $R^2_e$ es el $R^2$ de la regresiÃ³n

$$e\_i^2 = \alpha\_0 + \alpha\_1 x\_{1i} + \alpha\_2 x\_{2i} + \cdots + \alpha\_k x\_{ki} + v\_i$$

Bajo la nula, $\text{LM}$ se distribuye asintÃ³ticamente $\chi^2_k$.

--

Este estadÃ­stico testeat H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_k = 0$.

Rechazar la hipotesis nula implica que hay evidencia de heterocedasticidad.

---
layout: true
# Testeando la heterocedasticidad
## La distribuciÃ³n $\chi^2$

---

Acabamos de mencionar que bajo el valor nulo, el estadÃ­stico de la prueba B-P se distribuye como una variable aleatoria $\chi^2$ con $k$ grados de libertad.

La distribuciÃ³n $\chi ^ 2$ es solo otro ejemplo de una distribuciÃ³n comÃºn (con nombre) (como la distribuciÃ³n Normal, la distribuciÃ³n $t$ y la $F$).

---

Tres ejemplos de $\chi_k^2$: $\color{#314f4f}{k = 1}$, $\color{#e64173}{k = 2}$, y $\color{orange}{k = 9}$

```{R, chisq1, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = tibble(x = c(0, 20)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 3),
    fill = red_pink, alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 3), n = 1e3,
    color = red_pink
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 9),
    fill = "orange", alpha = 0.3
  ) +
  stat_function(
    fun = dchisq, args = list(df = 9), n = 1e3,
    color = "orange"
  ) +
  labs(x = "x", y = "f") +
  theme_axes_math
```
---

Probabilidad de observar un estadÃ­stico mÃ¡s extremo $\widehat{\text{LM}}$ bajo H.sub[0]

```{R, chisq2, echo = F, dev = "svg", fig.height = 5.5}
ggplot(data = tibble(x = c(0, 8)), aes(x)) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = "darkslategrey", alpha = 0.05
  ) +
  geom_area(
    stat = "function", fun = dchisq, args = list(df = 2),
    fill = red_pink, alpha = 0.85,
    xlim = c(5, 8)
  ) +
  stat_function(
    fun = dchisq, args = list(df = 2), n = 1e3,
    color = "darkslategrey"
  ) +
  geom_vline(xintercept = 5, color = grey_dark, size = 0.5, linetype = "dotted") +
  annotate("text", x = 5, y = 1.55 * dchisq(5, df = 2), label = TeX("$\\widehat{LM}$"), size = 7) +
  labs(x = "x", y = "f") +
  theme_axes_math
```
---
layout: true
# Testeando la heterocedasticidad
## El test de Breusch-Pagan
---

**Problema:** Seguimos asumiendo una .hi[forma funcional] bastante restrictiva entre nuestras variables explicativas $X$ y las variaciones de nuestras perturbaciones $\sigma ^ 2_i$.

--

**Resultado:** B-P *puede* aÃºn pasar por alto formas bastante simples de heterocedasticidad.

---

Las pruebas de Breusch-Pagan siguen siendo .hi[sensibles a la forma funcional].

```{R, bp1, echo = F, dev = "svg", fig.height = 3.75}
set.seed(12345)
# Data
bp_df <- tibble(
  x = runif(1e3, -3, 3),
  e = rnorm(1e3, 0, sd = 2 + x^2),
  y = 1 + 3 * x + e
)
# Regressions
lm_bp1 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
lm_bp2 <- lm(residuals(lm(y ~ x, bp_df))^2 ~ 1 + bp_df$x + I(bp_df$x^2)) %>%
  summary() %$% r.squared %>% multiply_by(1e3)
# The figure
ggplot(data = bp_df, aes(x = x, y = e)) +
geom_point(size = 2.75, color = "darkslategrey", alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

$$
\begin{aligned}
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} & \widehat{\text{LM}} &= `r round(lm_bp1, 2)` &\mathit{p}\text{-value} \approx `r round(pchisq(lm_bp1, 1, lower.tail = F), 3)` \\
  e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} \color{#e64173}{+ \hat{\alpha}_2 x^2_{1i}} & \widehat{\text{LM}} &= `r round(lm_bp2, 2)` &\mathit{p}\text{-value} < 0.001
\end{aligned}
$$
---
layout: true
# Testeando la heterocedasticidad
## El test de White
---

Hasta ahora hemos estado probando relaciones especÃ­ficas entre nuestras variables explicativas y las varianzas de las perturbaciones, por ejemplo,

- H.sub[0]: $\sigma_1^2 = \sigma_2^2$ para dos grupos basados en $x_j$ (**G-Q**)

- H.sub[0]: $\alpha_1 = \cdots = \alpha_k = 0$ de $e_i^2 = \alpha_0 + \alpha_1 x_{1i} + \cdots + \alpha_k x_{ki} + v_i$ (**B-P**)

--

Sin embargo, en realidad queremos saber si

$$\sigma_1^2 = \sigma_2^2 = \cdots = \sigma_n^2$$

**P:** Â¿No podemos simplemente probar esta hipÃ³tesis?

--

**R:** MÃ¡s o menos.
---

Con este objetivo, Hal White aprovechÃ³ el hecho de que podemos .hi[reemplazar el requisito de homocedasticidad con una suposiciÃ³n mÃ¡s dÃ©bil]:

- **Viejo:** $\mathop{\text{Var}} \left( u_i \middle| X \right) = \sigma^2$

- **Nuevo:** $u^2$ estÃ¡ *incorrelacionado* con las variables explicativas (_por ejemplo_,  $x_j$ para todo $j$), sus cuadrados (_por ejemplo_, $x_j^2$), y las interacciones d eprimer grado (_por ejemplo_, $x_j x_h$).

--

Este nuevo supuesto es mÃ¡s fÃ¡cil de probar explÃ­citamente (*pista:* regresiÃ³n).
---

Un resumen de la prueba de White para heterocedasticidad:

.pseudocode-small[

1\. Regresamos y en x.sub[1], x.sub[2], â€¦, x.sub[k]. Guardo los residuos e.

2\. los residuos al cuadrado de todas las variables explicativas, sus cuadrados e interacciones.

3\. Guardo R.sub[e].super[2].

4\. Calculo el estadÃ­stico del test para testear H.sub[0]: $\alpha_p = 0$ para todo $p\neq0$.

]
---

Al igual que con la prueba de Breusch-Pagan, la estadÃ­stica de prueba de White es

$$\text{LM} = n \times R_e^2 \qquad \text{Bajo H}_0,\, \text{LM} \overset{\text{d}}{\sim} \chi_k^2$$

pero ahora $R^2_e$ proviene de la regresiÃ³n de $e^2$ sobre las variables explicativas, sus cuadrados y sus interacciones.


**Nota:** El $k$ (para nuestro $\chi_k^2$) es igual al nÃºmero de parÃ¡metros estimados en la regresiÃ³n de arriba 
(el $\alpha_j$), excluyendo el intercepto $\left( \alpha_0 \right)$.
---

**Nota prÃ¡ctica:** Si una variable es igual a su cuadrado (_ejemplo_, variables binarias), entonces no puede incluirla. La misma regla se aplica a las interacciones.
---

*Ejemplo:* Considere el modelo.super[â€ ] $y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + u$

**Paso 1:** Estime el modelo; obtenga los residuos $(e)$.

**Paso 2:** Regrese $e^2$ en las variables explicativas, sus cuadrados e interacciones.
$$
\begin{aligned}
  e^2 = \
  & \alpha_0 + \alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 + \alpha_4 x_1^2 + \alpha_5 x_2^2 + \alpha_6 x_3^2 \\
  &+ \alpha_7 x_1 x_2 + \alpha_8 x_1 x_3 + \alpha_9 x_2 x_3 + v
\end{aligned}
$$

Guarde el R.super[2] de esta ecuaciÃ³n (llamÃ©moslo, $R_e^2$).

**Paso 3:** Testeo H.sub[0]: $\alpha_1 = \alpha_2 = \cdots = \alpha_9 = 0$ using $\text{LM} = n R^2_e \overset{\text{d}}{\sim} \chi_9^2$.

.footnote[
[â€ ]: Para simplificar la notaciÃ³n, no utilizo los subÃ­ndices $i$.
]
---

```{R, white1, echo = F, dev = "svg", fig.height = 4}
set.seed(12345)
# The figure
ggplot(data = bp_df, aes(x = x, y = e)) +
geom_point(size = 2.75, color = "darkslategrey", alpha = 0.5) +
labs(x = "x", y = "u") +
theme_axes_math
```

Ya hicimos la prueba de White para esta regresiÃ³n lineal simple.

$$
\begin{aligned}
 e_i^2 &= \hat{\alpha}_0 + \hat{\alpha}_1 x_{1i} \color{#e64173}{+ \hat{\alpha}_2 x^2_{1i}} & \widehat{\text{LM}} &= `r round(lm_bp2, 2)` &\mathit{p}\text{-value} < 0.001
\end{aligned}
$$
---
layout: true
# Heterocedasticidad
## Preguntas de repaso
---

--

- **P:** Â¿CuÃ¡l es la definiciÃ³n de heterocedastidad?

- **P:** Â¿Por quÃ© nos preocupa la heterocedastidad?

- **P:** Â¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Â¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Dado que no podemos observar $u_i$'s, Â¿quÃ© podemos usar para *aprender mÃ¡s* sobre la heterocedasticidad?

- **P:** Â¿QuÃ© test se recomiendo para la heterocedasticidad? Â¿Por quÃ©?

---
count: false

- **P:** Â¿CuÃ¡l es la definiciÃ³n de heterocedastidad?
--

- **R:**
<br>.hi[MatemÃ¡tica:] $\mathop{\text{Var}} \left( u_i | X \right) \neq \mathop{\text{Var}} \left( u_j | X \right)$ para algÃºn $i\neq j$.
<br>.hi[Palabras:] Hay una relaciÃ³n sistemÃ¡tica entre la varianza de $u_i$ y nuestras variables explicativas.
---
count: false

.grey-vlight[

- **P:** Â¿CuÃ¡l es la definiciÃ³n de heterocedastidad?

]

- **P:** Â¿Por quÃ© nos preocupa la heterocedastidad?
--

- **R:** Esto sesga nuestros errores estÃ¡ndar, arruinando nuestras pruebas estadÃ­sticas e intervalos de confianza. AdemÃ¡s: MCO ya no es el MELI.
---
count: false

.grey-vlight[

- **P:** Â¿CuÃ¡l es la definiciÃ³n de heterocedastidad?

- **P:** Â¿Por quÃ© nos preocupa la heterocedastidad?

]

- **P:** Â¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?
--

- **R:** No es exactamente lo que queremos, pero dado que $y$ es una funciÃ³n de $x$ y $u$, aÃºn puede ser informativo. Si $y$ se vuelve mÃ¡s/menos disperso a medida que cambia $x$, es probable que tengamos heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** Â¿CuÃ¡l es la definiciÃ³n de heterocedastidad?

- **P:** Â¿Por quÃ© nos preocupa la heterocedastidad?

- **P:** Â¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

]

- **P:** Â¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?
--

- **R:** Si. El margen de $e$ representa su varianza y nos dice algo sobre la varianza de $u$. Las tendencias en esta varianza, a lo largo de $x$, sugieren heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** Â¿CuÃ¡l es la definiciÃ³n de heterocedastidad?

- **P:** Â¿Por quÃ© nos preocupa la heterocedastidad?

- **P:** Â¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Â¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

]

- **P:** Dado que no podemos observar $u_i$'s, Â¿quÃ© podemos usar para *aprender mÃ¡s* sobre la heterocedasticidad?
--

- **R:** Usamos los $e_i$ para predecir/aprender sobre los $u_i$. Este truco es clave para casi todo lo que hacemos con la prueba/correcciÃ³n de heterocedasticidad.
---
count: false

.grey-vlight[

- **P:** Â¿CuÃ¡l es la definiciÃ³n de heterocedastidad?

- **P:** Â¿Por quÃ© nos preocupa la heterocedastidad?

- **P:** Â¿Graficar $y$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Â¿Graficar $e$ contra $x$, nos dice algo sobre la heterocedastidad?

- **P:** Dado que no podemos observar $u_i$'s, Â¿quÃ© podemos usar para *aprender mÃ¡s* sobre la heterocedasticidad?

]


---
layout: true
# Heterocedasticidad - ejercicio

---
class: inverse, middle
---
Â¿En quÃ© subfiguras de abajo probablemente $u_i$ es heterocedÃ¡stico? Expliquen brevemente. 
(***Ayuda*** Puede haber mÃ¡s de una.)

**Figura 1**
```{R, echo = F, dev = "svg", fig.height = 2.5}
set.seed(123)
n <- 101
# No violations
p1 <- ggplot(data = tibble(x = 1:n, u = rnorm(n)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1a") +
theme_axes
# Violates homoskedasticity
p2 <- ggplot(data = tibble(x = 1:n, u = rnorm(n, sd = abs(sin(x/(100))) + 0.1)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1b") +
theme_axes
# Violates both
p3 <- ggplot(data = tibble(x = 1:n, u = runif(n, min = -250, max = (x-50.5)^2)), aes(x = x, y = u)) +
geom_hline(yintercept = 0) +
scale_y_continuous(breaks = 0) +
geom_point() +
ggtitle("Figura 1c") +
theme_axes
# Put it all together
grid.arrange(p1, p2, p3, nrow = 1)
```

---


.pink[
**Respuesta:** $u_i$ es probablemente heterocedÃ¡stico en las subfiguras **1b** y **1c**. Podemos ver tendencias claras (relaciones) entre la varianza de $u_i$ (su dispersiÃ³n) y $x_i$.
]

---
**1b.** En la presencia de heterocedasticidad, Â¿MCO sigue siendo insesgado?

--
.pink[
**Respuesta:** SÃ­.
]


**1c.** Â¿QuÃ© problemas causa la heterocedasticidad en nuestro setting de MCO?
--

.pink[
**Respuesta** La heterocedasticidad hace a (1) MCO ineficiente y (2) sesga la estimaciÃ³n de los errores estÃ¡ndar.
]

**1d.** Imaginemos que queremos estimar por MCO este modelo

$$
\begin{align}
  y_i = \beta_0 + \beta_1 x_i + u_i \tag{1}
\end{align}
$$

donde $x_i$ es una variable categÃ³rica que toma los valores $1$, $2$, o $3$.

Supongamos que sabemos que $\mathop{\text{Var}} \left( u_i \middle| x_i = 1 \right) = 15$ y $\mathop{\text{Var}} \left( u_i \middle| x_i = 2 \right) = 15$. No conocemos $\mathop{\text{Var}} \left( u_i \middle| x_i = 3 \right)$, _i.e._, $\mathop{\text{Var}} \left( u_i | x_i = 3 \right) = \sigma_3^2$ para algÃºn parÃ¡metro desconocido $\sigma_3^2$.

Â¿QuÃ© valor deberÃ­a tomar $\sigma_3^2$ para que nuestro modelo sea homocedÃ¡stico?

--

.pink[
**Respuesta** Para que nuestro modelo sea homocedÃ¡stico, $\sigma_3^2 = 15$.
]
---

**1e.** *Goldfeld-Quandt* Para probar si los datos que usaremos para estimar la ecuaciÃ³n $(1)$ son homocedÃ¡sticos / heteroscedÃ¡sticos, haremos una prueba de Goldfeld-Quandt. 

Estimamos $(1)$ para el tercio superior del conjunto de datos (ordenados en $x$) y encontramos SSE.sub [3] = 100. Estimamos $(1)$ en el tercio medio y encontramos SSE.sub [2] = 80. Finalmente, estimamos $(1)$ en el tercio inferior y encontramos SSE.sub [1] = 70. Cada uno de estos tres grupos tiene 100 observaciones. Realice una prueba de Goldfeld-Quandt. Exprese sus hipÃ³tesis, calcule el estadÃ­stico de la prueba G-Q, determine el valor *p*, concluya.

***Sugerencia:*** La funciÃ³n `pf(q, df1, df2, lower.tail = F)` calcula la probabilidad de observar un valor de `q` o uno mayor en una distribuciÃ³n $F$ con `df1, df2` grados de libertad numerador y denominador.

--

.pink[
**Respuesta** La hipotesis para nuestro test es

.b[H.sub[o]]: $\sigma_1^2 = \sigma_3^2$ (homocedasticidad) *vs.* .b[H.sub[a]]: $\sigma_1^2 \neq \sigma_3^2$ (heterocedasticidad)

Para la prueba de Goldfeld-Quandt, probamos esta hipÃ³tesis nula utilizando el estadÃ­stico de prueba

$$
\begin{align}
   F = \dfrac{SSE_3}{SSE_1} = \dfrac{100}{70} \approx 1.4286
\end{align}
$$

Bajo la hipÃ³tesis nula, este estadÃ­stico de prueba tiene una distribuciÃ³n $F$ con 98 (= 100-2) grados de libertad en el numerador y denominador. Usando .mono[R] podemos calcular el *p*-valor:

```{R, key-2f}
# p-valor
pf(100/70, df1 = 100-2, df2 = 100-2, lower.tail = F)
```

Este valor *p* es menor que 0.05, por lo que rechazamos la hipÃ³tesis nula y concluimos que hay evidencia estadÃ­sticamente significativa de heterocedasticidad (al nivel del 5 por ciento).
]
---
layout: true
# Viviendo con heterocedasticidad
---
class: inverse, middle, true
---

EÂ¡n la presencia de heterocedasticidad, MCO es

- todavÃ­a .hi[insesgado]
- pero... .hi[ya no es mÃ¡s eficiente]

En promedio, obtenemos la respuesta correcta pero con mÃ¡s ruido (menos precisiÃ³n).
<br> *AdemÃ¡s:* Nuestros errores estÃ¡ndar estÃ¡n sesgados.

--

**Opciones:**

1. Compruebe la .hi[especificaciÃ³n] de la regresiÃ³n.
2. Encuentre un nuevo y mÃ¡s eficiente .hi[estimador insesgado] para $\beta_j$'s.
3. Vivir con la ineficiencia de MCO; encontrar un .hi[nuevo estimador de varianza].
  - Errores estÃ¡ndar
  - Intervalos de confianza
  - Pruebas de hipÃ³tesis
---
layout: true
# Viviendo con heterocedasticidad
## Error de especificaciÃ³n
---

Como hemos comentado, la especificaciÃ³n .pink[<sup> â€  </sup>] del modelo de regresiÃ³n es muy importante para la insesgadez y la eficiencia de su estimador.

**Respuesta \#1:** AsegÃºrense de que su especificaciÃ³n no cause heterocedasticidad.

.footnote[.pink[â€ ] *EspecificaciÃ³n:* Forma funcional y variables incluidas.]
---

*Ejemplo:* Dejemos que la relacion poblacional sea $$y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + u_i$$

con $\mathop{\boldsymbol{E}}\left[ u_i \middle| x_i \right] = 0$ y $\mathop{\text{Var}} \left( u_i \middle| x_i \right) = \sigma^2$.

Sin embargo, omitimos $x^2$ y estimamos $$y_i = \gamma_0 + \gamma_1 x_i + w_i$$

Entonces $$w_i = u_i + \beta_2 x_i^2 \implies \mathop{\text{Var}} \left( w_i \right) = f(x_i)$$

_Es decir_, la varianza de $w_i$ cambia sistemÃ¡ticamente con $x_i$ (heterocedasticidad).
---

```{R, spec data, include = F}
# Set the seed
set.seed(1234)
# Generate data
spec_df <- tibble(x = runif(1e3, 0, 3), y = exp(0.5 + 0.6 * x + rnorm(1e3, sd = 0.3)))
# Add residuals ('w': wrong specification; 'c': correct specification)
spec_df %<>% dplyr::mutate(
  e_w = lm(y ~ x, data = spec_df) %>% residuals(),
  e_c = lm(log(y) ~ x, data = spec_df) %>% residuals()
)
```

.pink[Verdad:] $\color{#e64173}{\log\left(y_i\right) = \beta_0 + \beta_1 x_i + u_i}$ â€ƒ.slate[**Error de especificaciÃ³n:**] $\color{#314f4f}{y_i = \beta_0 + \beta_1 x_i + v_i}$

```{R, spec plot1, echo = F, dev = "svg", fig.height = 5}
ggplot(data = spec_df, aes(x = x)) +
  geom_point(aes(y = e_w), color = "darkslategrey", size = 2.75, alpha = 0.5, shape = 16) +
  geom_point(aes(y = e_c), color = red_pink, size = 2.5, alpha = 0, shape = 19) +
  labs(x = "x", y = "e") +
  theme_axes_math
```
---

.pink[**Verdad:**] $\color{#e64173}{\log\left(y_i\right) = \beta_0 + \beta_1 x_i + u_i}$ â€ƒ.slate[Error de especificaciÃ³n:] $\color{#314f4f}{y_i = \beta_0 + \beta_1 x_i + v_i}$

```{R, spec plot2, echo = F, dev = "svg", fig.height = 5}
ggplot(data = spec_df, aes(x = x)) +
  geom_point(aes(y = e_w), color = "darkslategrey", size = 2.75, alpha = 0.25, shape = 1) +
  geom_point(aes(y = e_c), color = red_pink, size = 2.5, alpha = 0.5, shape = 19) +
  labs(x = "x", y = "e") +
  theme_axes_math
```
---

De manera mÃ¡s general:

**El problema del error de especificaciÃ³n:** La incorrecta especificaciÃ³n del modelo de regresiÃ³n puede causar (entre otros problemas).

--

** SoluciÃ³n: ** ðŸ’¡ Lo hacemos bien bien (_ejemplo_, no omiten $x^2$).

--

**Nuevos problemas:**

- A menudo no conocemos la especificaciÃ³n *correcta*.
- Nos gustarÃ­a un proceso mÃ¡s formal para abordar la heterocedasticidad.

--

**ConclusiÃ³n:** La especificaciÃ³n a menudo no "resolverÃ¡" la heterocedasticidad.
<br> Sin embargo, especificar correctamente su modelo sigue siendo realmente importante.

---
layout: true
# Viviendo con heterocedasticidad
## MÃ­nimos Cuadrados Ponderados
---

MÃ­nimos Cuadrados Ponderados (MCP) presentan otro enfoque.

**Respuesta \#2:** Aumenta la eficiencia ponderando nuestras observaciones.

--

Sea la verdadera relaciÃ³n de poblaciÃ³n

$$
\begin{align}
  y_i = \beta_0 + \beta_1 x_{i} + u_i \tag{1}
\end{align}
$$

con $u_i \sim \mathop{N} \left( 0,\, \sigma_i^2 \right)$.

--

Ahora transformamos $(1)$ dividiendo cada observaciÃ³n por $\sigma_i$, _es decir_,

$$
\begin{align}
  \dfrac{y_i}{\sigma_i} &= \beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{x_{i}}{\sigma_i} + \dfrac{u_i}{\sigma_i} \tag{2}
\end{align}
$$

---

$$
\begin{align}
  y_i &= \beta_0 + \beta_1 x_{i} + u_i \tag{1} \\[1em]
  \dfrac{y_i}{\sigma_i} &= \beta_0 \dfrac{1}{\sigma_i} + \beta_1 \dfrac{x_{i}}{\sigma_i} + \dfrac{u_i}{\sigma_i} \tag{2}
\end{align}
$$

Mientras que $(1)$ sea heterocedÃ¡stico,
--
 $\color{#e64173}{(2)}$ .hi[es homocedÃ¡stico].

âˆ´ MCO es eficiente e insesgado para estimar los $\beta_k$ en $(2)$!

--

Â¿Por quÃ© $(2)$ es homocedÃ¡stico?

--

$\mathop{\text{Var}} \left( \dfrac{u_i}{\sigma_i} \middle| x_i \right) =$
--
 $\dfrac{1}{\sigma_i^2} \mathop{\text{Var}} \left( u_i \middle| x_i \right) =$
--
 $\dfrac{1}{\sigma_i^2} \sigma_i^2 =$
--
 $1$
---

MCP es genial, pero necesitamos saber $\sigma_i^2$, que generalmente es poco probable.

Podemos relajar *ligeramente* este requisito, en lugar de requerir

1. $\mathop{\text{Var}} \left(u_i | x_i \right) = \sigma_i^2 = \sigma^2h(x_i)$

2. Conocemos $h(x)$.

--

Como antes, transformamos nuestro modelo heterocedÃ¡stico en un modelo homocedÃ¡stico. Esta vez dividimos los datos de cada observaciÃ³n <sup>.pink[â€ ]</sup> por $\sqrt{h(x_i)}$.

.note[
.pink[â€ ] Divide *todos* los datos por $\sqrt{h(x_i)}$, incluida la constante.
]
---

$$
\begin{align}
  y_i &= \beta_0 + \beta_1 x_{i} + u_i \tag{1} \\[1em]
  \dfrac{y_i}{\sqrt{h(x_i)}} &= \beta_0 \dfrac{1}{\sqrt{h(x_i)}} + \beta_1 \dfrac{x_{i}}{\sqrt{h(x_i)}} + \dfrac{u_i}{\sqrt{h(x_i)}} \tag{2}
\end{align}
$$
con $\mathop{\text{Var}} \left( u_i | x_i \right) = \sigma^2 h(x_i)$.

--

Ahora chequeemos que $(2)$ sea efectivamente homocedÃ¡stico.

$\mathop{\text{Var}} \left( \dfrac{u_i}{\sqrt{h(x_i)}} \middle| x_i \right) =$
--
 $\dfrac{1}{h(x_i)} \mathop{\text{Var}} \left( u_i \middle| x_i \right) =$
--
 $\dfrac{1}{h(x_i)} \sigma^2 h(x_i) =$
--
 $\color{#e64173}{\sigma^2}$

.hi[Â¡Homocedasticidad!]
---

Los estimadores por .hi[MÃ­nimos Cuadrados Ponderados] (MCP) son una clase especial de los estimadores estimators .hi[MÃ­nimos Cuadrados Generalizados] (MCG) pero enfocados en heterocedasticidad.

--

$$
  y\_i = \beta\_0 + \beta\_1 x\_{1i} + u\_i \quad \color{#e64173}{\text{ vs. }} \quad
  \dfrac{y\_i}{\sigma\_i} = \beta\_0 \dfrac{1}{\sigma\_i} + \beta\_1 \dfrac{x\_{1i}}{\sigma\_i} + \dfrac{u\_i}{\sigma\_i}
$$

*Notas:*

1. MCP **transforma** un modelo heterocedÃ¡stico en un modelo homocedÃ¡stico.
2. **Ponderando:** MCP le da peso menor a las observaciones con mayor varianza de $u_i$'s.
3. **Gran requisito:** MCP requiere que *sepamos* $\sigma_i^2$ para cada observaciÃ³n.
4. MCP es generalmente **inviable**. *Factible* MCGF ofrecen una soluciÃ³n.
5. Bajo sus supuestos: MCP es el **mejor estimador lineal insesgado**.
---
layout: true
# Viviendo con heterocedasticidad
## Errores estÃ¡ndar robustos-heterocedÃ¡sticos
---

**Respuesta \#3:**

- Ignoramos la ineficiencia de MCO (en presencia de heterocedasticidad).
- Nos centramos en .hi[estimaciones no sesgadas de nuestros errores estÃ¡ndar].
- En el proceso: Inferencia correcta ðŸ˜Ž

--

**P:** Â¿QuÃ© es un error estÃ¡ndar?
--

<br>**R:**  La .hi[desviaciÃ³n estÃ¡ndar de la distribuciÃ³n de un estimador].

--

Los estimadores (como $\hat{\beta}_1$) son variables aleatorias, por lo que tienen distribuciones.

Los errores estÃ¡ndar nos dan una idea de cuÃ¡nta variabilidad hay en nuestro estimador.

---

*Recuerden*: podemos escribir el estimador de MCO para $\beta_1$ como

$$\hat{\beta}_1 = \beta\_1 + \dfrac{\sum_i \left( x_i - \overline{x} \right) u_i}{\sum_i \left( x_i - \overline{x} \right)^2} = \beta_1 + \dfrac{\sum_i \left( x_i - \overline{x} \right) u_i}{\text{SCT}_x} \tag{3}$$

--

Sea $\mathop{\text{Var}} \left( u_i \middle| x_i \right) = \sigma_i^2$.

--

Podemos usar $(3)$ para ecsribir la varianza de $\hat{\beta}_1$, _es decir_,

$$\mathop{\text{Var}} \left( \hat{\beta}_1 \middle| x_i \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 \sigma_i^2}{\text{SCT}_x^2} \tag{4}$$
---

Si queremos estimaciones insesgadas para nuestros errores estÃ¡ndar, necesitamos una estimaciÃ³n no sesgada para

$$\dfrac{\sum_i \left( x_i - \overline{x} \right)^2 \sigma_i^2}{\text{SCT}_x^2}$$

Nuestro viejo amigo (?) Hal White nos dio tal estimador:.pink[<sup>â€ </sup>]

$$\widehat{\mathop{\text{Var}}} \left( \hat{\beta}_1 \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 e_i^2}{\text{SCT}_x^2}$$

donde $e_i$ viene de la regresiÃ³n MCO de interÃ©s.

.footnote[
.pink[â€ ] Esta ecuaciÃ³n especÃ­fica es para regresiÃ³n lineal simple.
]
---

Nuestra estimaciÃ³n de errores estÃ¡ndar robustos-heterocedÃ¡sticos para $\beta_j$.

.hi[Caso 1] RegresiÃ³n lineal simple, $y_i = \beta_0 + \beta_1 x_i + u_i$

$$\widehat{\mathop{\text{Var}}} \left( \hat{\beta}_1 \right) = \dfrac{\sum_i \left( x_i - \overline{x} \right)^2 e_i^2}{\text{SCT}_x^2}$$

.hi[Caso 2] RegresiÃ³n lineal mÃºltiple, $y_i = \beta_0 + \beta_1 x_{1i} + \cdots + \beta_k x_{ki} + u_i$

$$\widehat{\mathop{\text{Var}}} \left( \hat{\beta}_j \right) = \dfrac{\sum_i \hat{r}_{ij}^2 e_i^2}{\text{SCT}_{x_j^2}}$$

donde $\hat{r}_{ij}$ denota el i.super[Ã©simo] residual producto de regresar $x_j$ contra las demÃ¡s variables explicativas.
---

Con estos errores estÃ¡ndar, podemos volver hacer inferencia correctamente.

_Ejemplo_, podemos actualizar nuestra fÃ³rmula del estadÃ­stico $t$ con nuestro nuevo error estÃ¡ndar robusto a la heterocedasticidad.

$$t = \frac{\text{EstimaciÃ³n Puntual - Valor Hipotetizado}}{\text{Error estÃ¡ndar}}$$
---
layout: false
class: inverse, middle

# Viviendo con heterocedasticidad
## Ejemplos
---
layout: true
# Viviendo con heterocedasticidad
---

## Ejemplos

De nuevo al set de datos de puntajes en pruebas...

```{R, ex test data}
# Load packages
library(Ecdat)
library(tidyverse)
# Select y rename desired variables; assign to new dataset; format as tibble
test_df <- Caschool %>% select(
  test_score = testscr, ratio = str, income = avginc, enrollment = enrltot
) %>% as_tibble()
# View first 2 rows of the dataset
head(test_df, 2)
```
---
layout: true
# Viviendo con heterocedasticidad
## Ejemplo: Error de especificaciÃ³n
---

Encontramos evidencia significativa de heterocedasticidad.

Comprobemos si se debiÃ³ a una especificaciÃ³n incorrecta de nuestro modelo.
---

Modelo.sub[1]: $\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$
<br>`lm(test_score ~ ratio + income, data = test_df)`


```{R, ex spec1, echo = F, dev = "svg", fig.height = 4.75}
# Modelo 1: test ~ ratio + income
test_df %<>% mutate(e1 = lm(test_score ~ ratio + income, data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income, y = e1)) +
geom_point(size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Ingreso", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Modelo.sub[2]: $\log\left(\text{Puntaje}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$
<br>`lm(log(test_score) ~ ratio + income, data = test_df)`


```{R, ex spec2, echo = F, dev = "svg", fig.height = 4.75}
# Modelo 1: test ~ ratio + income
test_df %<>% mutate(e2 = lm(log(test_score) ~ ratio + income, data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income)) +
geom_point(aes(y = e2), size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Ingreso", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Modelo.sub[3]: $\log\left(\text{Puntaje}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \log\left(\text{Ingreso}_i\right) + u_i$
<br>`lm(log(test_score) ~ ratio + log(income), data = test_df)`


```{R, ex spec3, echo = F, dev = "svg", fig.height = 4.75}
# Modelo 1: test ~ ratio + income
test_df %<>% mutate(e3 = lm(log(test_score) ~ ratio + log(income), data = test_df) %>% residuals())
# Plot
ggplot(data = test_df, aes(x = income)) +
geom_point(aes(y = e3), size = 3, alpha = 0.5, color = red_pink) +
labs(x = "Ingreso", y = TeX("\\textit{e}")) +
theme_axes_serif
```
---

Testeamos esta nueva especificaciÃ³n con el test de White:

.center[
Modelo.sub[3]: $\log\left(\text{Puntaje}_i\right) = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \log\left(\text{Ingreso}_i\right) + u_i$
]

```{R, ex spec test, include = F}
white_r2_spec <- lm(e3^2 ~
  ratio * log(income) + I(ratio^2) + I(log(income)^2),
  data = test_df
) %>% summary() %$% r.squared
white_stat_spec <- white_r2_spec %>% multiply_by(420)
```

--
La regresiÃ³n para el test de White 

--
$$\begin{align}
  e_i^2 = &\alpha_0 + \alpha_1 \text{Ratio}_i + \alpha_2 \log\left(\text{Ingreso}_i\right) + \alpha_3 \text{Ratio}_i^2 + \alpha_4 \left(\log\left(\text{Ingreso}_i\right)\right)^2 \\
  &+ \alpha_5 \left(\text{Ratio}_i\times\log\left(\text{Ingreso}_i\right)\right) + v_i
\end{align}$$
--
con un $R_e^2\approx`r round(white_r2_spec, 3)`$
--
 y el estadÃ­stico t
--
 $\widehat{\text{LM}} = n\times R_e^2 \approx `r round(white_stat_spec, 1)`$.

--

Bajo H.sub[0], $\text{LM}$ se distribuye
--
 $\chi_5^2$
--
 $\implies$ *p*-valor $\approx$ `r pchisq(white_stat_spec, 5, lower.tail = F) %>% round(3)`.

--

âˆ´
--
 .hi[Rechazamos H.sub[0].]

--
 .hi[ConclusiÃ³n:]

--

Existe evidencia estadÃ­sticamente significativa de heterocedasticidad al nivel del cinco por ciento.

---

De acuerdo, intentamos ajustar nuestra especificaciÃ³n, pero todavÃ­a hay evidencia de heterocedasticidad.

**Siguiente:** En general, vamos a ir por los errores estÃ¡ndar robustos.

- MCO sigue siendo insesgado para los .hi[coeficientes] (los $\beta_j$'s)

- los errores estÃ¡ndar robustos son insesgados para los .hi[errores estÃ¡ndar] de los $\hat{\beta}_j$'s, _es decir_, $\sqrt{\mathop{\text{Var}} \left( \hat{\beta}_j \right)}$.
---
layout: true
# Viviendo con heterocedasticidad
## Ejemplos: Errores estÃ¡ndar robustos
---

Volvemos a nuestro modelo

$$\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$$

Podemos usar el paquete `lfe` en .mono[R] para calcular los errores estÃ¡ndar.
---

$$ \text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i $$

1\. Corremos la regresiÃ³n con `felm()` (en vez de `lm()`)
```{R, lfe1}
# Load 'lfe' package
library(lfe)
# Regress log score on ratio y log income
test_reg <- felm(test_score ~ ratio + income, data = test_df)
```

--

*(!)* `felm()` usa la misma sintÃ¡xis de regresiÃ³n que `lm()`.
---

$$\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$$

2\. Estimamos los errores estÃ¡ndar robustos con la opciÃ³n `robust = T` en `summary()`
```{R, lfe2a, eval = F}
# Het-robust standard errors con 'robust = T'
summary(test_reg, robust = T)
```
```{R, lfe2b, echo = F}
test_het_out <- summary(test_reg, robust = T) %>% capture.output()
test_het_out[10:13] %>% paste0("\n") %>% cat()
```
---

Coeficientes y **errores estÃ¡ndar robustos**:
```{R, lfe3, eval = F}
summary(test_reg, robust = T)
```
```{R, lfe4, echo = F}
test_het_out <- summary(test_reg, robust = T) %>% capture.output()
test_het_out[10:13] %>% paste0("\n") %>% cat()
```

Coeficiencientes y **errores estÃ¡ndar MCO** (asumiendo homocedasticidad):
```{R, lfe5, eval = F}
summary(test_reg, robust = F)
```
```{R, lfe6, echo = F}
test_hom_out <- summary(test_reg, robust = F) %>% capture.output()
test_hom_out[10:13] %>% paste0("\n") %>% cat()
```
---
layout: true
# Viviendo con heterocedasticidad
## Ejemplo: MCP
---

Mencionamos que MCP no es posible muchas veces porque debemos saber la forma funcional de heterocedasticidad

**A**\. $\sigma_i^2$

o

**B**\. $h(x_i)$, donde $\sigma_i^2 = \sigma^2 h(x_i)$

--

*Hay* ocasiones en que podemos conocer $h(x_i)$.
---

ImagÃ­nense que los individuos de una poblaciÃ³n tienen perturbaciones homocedÃ¡sticass.

Sin embargo, en lugar de observar los datos de las personas, observamos (en los datos) los promedios de los grupos (por ejemplo, ciudades, condados, distritos escolares).

Si estos grupos tienen diferentes tamaÃ±os, entonces nuestro conjunto de datos serÃ¡ heterocedÃ¡stico, de una manera predecible.

**Recuerden:** La varianza de la media muestral depende del tamaÃ±o de la muestra,

$$\mathop{\text{Var}} \left( \overline{x} \right) = \dfrac{\sigma_x^2}{n}$$

--

**Ejemplo:**  Los datos de las pruebas de nuestra escuela se promedian a nivel escolar.
---

*Ejemplo:*  Los datos de las pruebas de nuestra escuela se promedian a nivel escolar.

Incluso si los estudiantes tuvieran perturbaciones homocedÃ¡sticas, las escuelas podrÃ­an tener perturbaciones heterocedÃ¡sticas, _por ejemplo_,

**Modelo a nivel individual:** $\quad \text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$

**Modelo a nivel escuela:** $\quad \overline{\text{Puntaje}}_s = \beta_0 + \beta_1 \overline{\text{Ratio}}_s + \beta_2 \overline{\text{Ingreso}}_s + \overline{u}_s$

donde el subÃ­ndice $s$ denota el una escuela.

$$\mathop{\text{Var}} \left( \overline{u}_s \right) = \dfrac{\sigma^2}{n_s}$$
---

Para MCP, buscamos una funciÃ³n $h(x_s)$ tal que $\mathop{\text{Var}} \left( \overline{u}_s | x_s \right) = \sigma^2 h(x_s)$.

--

Acabamos de mostrar<sup>.pink[â€ ]</sup> que $\mathop{\text{Var}} \left( \overline{u}_s |x_s \right) = \dfrac{\sigma^2}{n_s}$.

.footnote[
.pink[â€ ] Asumiendo que las perturbaciones individuales son homocedÃ¡sticos.
]

--

Entonces, $h(x_s) = 1/n_s$, donde $n_s$ es eÃ± nÃºmero de estudiantes en la escuela $s$.

--

Para implementar MCP, dividimos los datos de cada observaciÃ³n por $1/\sqrt{h(x_s)}$, lo que significa que necesitamos multiplicar los datos de cada escuela por $\sqrt{n_s}$.

--

La variable .mono[enrollment] (inscripciÃ³n) en el conjunto de datos .mono[test_df] es nuestro $n_s$.

---

Usando MCP para estimar $\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$

**Paso 1:** Multiplica cada variable por $1/ \sqrt{h(x_i)} = \sqrt{\text{Enrollment}_i}$

```{R, wls1}
# Create MCP transformed variables, multiplying by sqrt of 'pop'
test_df <- mutate(test_df,
  test_score_wls = test_score * sqrt(enrollment),
  ratio_wls      = ratio * sqrt(enrollment),
  income_wls     = income * sqrt(enrollment),
  intercept_wls  = 1 * sqrt(enrollment)
)
```

Observe que estamos creando una intersecciÃ³n transformada.
---

Usando MCP para estimar $\text{Puntaje}_i = \beta_0 + \beta_1 \text{Ratio}_i + \beta_2 \text{Ingreso}_i + u_i$

**Paso 2:** Corremos nuestra regresiÃ³n de MCP transformados

```{R, wls2}
# MCP regression
wls_reg <- lm(
  test_score_wls ~ -1 + intercept_wls + ratio_wls + income_wls,
  data = test_df
)
```
--
*Nota:* El `-1` en nuestra regresiÃ³n le dice a .mono[R] que no agregue una intersecciÃ³n, ya que estamos agregando una intersecciÃ³n transformada (`intercept_wls`).
---

Las .hi[estimaciones y sus errores estÃ¡ndar de MCP:]
```{R, wls3, echo = F}
# Grab the summary
test_wls_out <- summary(wls_reg) %>% capture.output()
# Print the coefficients
test_wls_out[11:14] %>% paste0("\n") %>% cat()
```
--
<br>
Las .hi[estimaciones MCO] y .hi[los errores robustos a la hetorocedasticidad:]
```{R, wls4, echo = F}
# Print the coefficients
test_het_out[10:13] %>% paste0("\n") %>% cat()
```

---
layout: true
# AutocorrelaciÃ³n

---
class: inverse, middle

---
# AutocorrelaciÃ³n
## Â¿QuÃ© es?

La .hi[autocorrelaciÃ³n] ocurre cuando nuestras perturbaciones estÃ¡n correlacionadas en el tiempo, _es decir_, $\mathop{\text{Cov}} \left( u_t,\, u_s \right) \neq 0$ para $t\neq s$.

--

Otra forma de pensar: si el *shock* de la perturbaciÃ³n $t$ se correlaciona con los shocks "cercanos" en $t-1$ y $t + 1$.

--

*Nota:* La **correlaciÃ³n serial** y la **autocorrelaciÃ³n** son lo mismo.

---
layout: false
# AutocorrelaciÃ³n
## MCO

Para **modelos estÃ¡ticos** o **modelos dinÃ¡micos con variables explicativas rezagadas**, en presencia de autocorrelaciÃ³n:

1. MCO proporciona estimaciones .pink[**insesgadas** para los coeficientes].

2. MCO crea .pink[estimaciones **sesgadas** para los errores estÃ¡ndar].

3. MCO es .pink[**ineficiente**.]

*Recuerden:* Las mismas implicancias que la heterocedasticidad.

